{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pkg_resources\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "if sym_spell.word_count:\n",
    "    pass\n",
    "else:\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# normalization\n",
    "def func_norm(s):\n",
    "    \"\"\"\n",
    "    Perform some basic normalisation operations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s:  str\n",
    "        text to operate on\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Normalised string\n",
    "    \n",
    "    \"\"\"\n",
    "    s = s.lower() # lower case\n",
    "    # letter repetition (>2)\n",
    "    s  = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n",
    "    # non word repetition\n",
    "    s = s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n",
    "    # noise text\n",
    "    s = re.sub(r' ing ', ' ', s)\n",
    "    # phrase repetition\n",
    "    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "def func_punc(w_list):\n",
    "    \"\"\"\n",
    "    Remove non-alphabet characters. Includes punctuation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        list of tokens to be processed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        list without non-alphabet characters\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word.isalpha()]\n",
    "\n",
    "def func_stopf(w_list):\n",
    "    \"\"\"\n",
    "    Remove stop words\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        list of tokens to be processed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        list without stop words\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    w_list  = [f for f in w_list if f not in stop_words]\n",
    "    return w_list\n",
    "\n",
    "# stemming\n",
    "pstem = PorterStemmer()\n",
    "\n",
    "def func_stem(w_list):\n",
    "    \"\"\"\n",
    "    stem word list\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        word list for stemming\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        stemmed word list \n",
    "    \"\"\"\n",
    "    sw_list = [pstem.stem(w) for w in w_list]\n",
    "    return sw_list\n",
    "\n",
    "# selecting nouns\n",
    "def func_noun(w_list):\n",
    "    \"\"\"\n",
    "    in: word list to be processed\n",
    "    out: w_list with only nouns selected\n",
    "    \"\"\"\n",
    "    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n",
    "\n",
    "# spell checker/typo correction\n",
    "def func_spell(w_list):\n",
    "    \"\"\"\n",
    "    in: word list to be processed\n",
    "    out: w_list with typo fixed by symspell. words with no match up will be dropped\n",
    "    \"\"\"\n",
    "    w_list_fixed = []\n",
    "    for word in w_list:\n",
    "        if 'covid' in word:\n",
    "            w_list_fixed.append(word)\n",
    "        else:\n",
    "            suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n",
    "            if suggestions:\n",
    "                w_list_fixed.append(suggestions[0].term)\n",
    "            else:\n",
    "                pass\n",
    "    return w_list_fixed\n",
    "\n",
    "def preprocess_sent(rw):\n",
    "    \"\"\"\n",
    "    Get sentence level preprocessed data from raw texts\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        rw: str\n",
    "            sentence to be processed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        sentence level pre-processed text\n",
    "    \"\"\"\n",
    "    s = func_norm(rw)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_word(s):\n",
    "    \"\"\"\n",
    "    Get word level preprocessed data from preprocessed sentences.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s:  str\n",
    "        sentence to be processed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        word level pre-processed text\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    w_list = word_tokenize(s)\n",
    "    w_list = func_punc(w_list)\n",
    "    w_list = func_noun(w_list)\n",
    "    w_list = func_spell(w_list)\n",
    "    w_list = func_stem(w_list)\n",
    "    w_list = func_stopf(w_list)\n",
    "\n",
    "    return w_list\n",
    "\n",
    "def preprocess(docs):\n",
    "    \"\"\"\n",
    "    Preprocess the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs: list\n",
    "        list of documents to be preprocessed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Preprocessed sentences, tokens\n",
    "    \"\"\"\n",
    "    print('Preprocessing raw texts ...')\n",
    "    #n_docs = len(docs)\n",
    "    sentences = []  # sentence level preprocessed\n",
    "    token_lists = []  # word level preprocessed\n",
    "    #samp = np.random.choice(n_docs)\n",
    "    for i in range(0, len(docs)):\n",
    "        sentence = preprocess_sent(docs.iloc[i])\n",
    "        token_list = preprocess_word(sentence)\n",
    "        if token_list:\n",
    "            sentences.append(sentence)\n",
    "            token_lists.append(token_list)\n",
    "        print('{} %'.format(str(np.round((i + 1) / len(writing_df.writing) * 100, 2))), end='\\r')\n",
    "    print('Preprocessing raw texts. Done!')\n",
    "    return sentences, token_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikit_posthocs as sp\n",
    "from scipy import stats\n",
    "from textblob import TextBlob\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def kruskal_wallis_func(in_df, group_col, test_col):\n",
    "    \"\"\"\n",
    "    Kruskal Wallis test and\n",
    "    post-hoc Dunn's.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd DataFrame\n",
    "        input dataframe\n",
    "    group_col:  str\n",
    "        name of group column\n",
    "    test_col:   str\n",
    "        name of column containing\n",
    "        relevant values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Statistic, pvalue\n",
    "    \"\"\"\n",
    "    data = in_df.pivot(columns = group_col, values = test_col)\n",
    "    if len(in_df[group_col].unique())>2:\n",
    "            statistic,pval = stats.kruskal(data.iloc[:,0],data.iloc[:,1],\n",
    "            data.iloc[:,2],nan_policy = 'omit')\n",
    "            posthoc = sp.posthoc_dunn(\n",
    "                        [data.iloc[:,0].dropna(),data.iloc[:,1].dropna(),data.iloc[:,2].dropna()],\n",
    "                        p_adjust = 'bonferroni'\n",
    "                        )\n",
    "            key = [data.columns[0],data.columns[1],data.columns[2]]\n",
    "    else:\n",
    "        statistic,pval = stats.kruskal(data.iloc[:,0],data.iloc[:,1],\n",
    "        nan_policy = 'omit')\n",
    "        posthoc = None\n",
    "        key = None\n",
    "    return statistic,pval, posthoc, key\n",
    "\n",
    "\n",
    "def get_sentiment(in_df, in_col):\n",
    "    \"\"\"\n",
    "    Get subjectivity\n",
    "    and polarity scores.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd DataFrame\n",
    "        DataFrame to operate on\n",
    "    in_col: str\n",
    "        column holding text data\n",
    "        to operate on\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Input DataFrame with\n",
    "    subjectivity/polarity columns\n",
    "    added.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Getting sentiment scores...\")\n",
    "    in_df = in_df.assign(\n",
    "                        polarity = in_df[in_col].astype('str').apply(\n",
    "                        [lambda x: TextBlob(x).sentiment.polarity]),\n",
    "                        subjectivity = in_df[in_col].astype('str').apply(\n",
    "                        [lambda x: TextBlob(x).sentiment.subjectivity])\n",
    "                        )\n",
    "    print(\"Done!\")\n",
    "    return in_df\n",
    "\n",
    "def run_mixedlm(in_df,group_name,formula, re_intercept):\n",
    "    \"\"\" \n",
    "    Run statsmodels LMEM.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pandas DataFrame\n",
    "        input dataframe\n",
    "    group_name: str\n",
    "        column to group by\n",
    "    formula:    str\n",
    "        patsy formula\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MixedLMResults instance\n",
    "    \"\"\"\n",
    "    if re_intercept:\n",
    "        model = smf.mixedlm(\n",
    "                            formula, in_df, groups = group_name,re_formula = re_intercept, missing = 'drop'\n",
    "                            ).fit()\n",
    "    else:\n",
    "        model = smf.mixedlm(formula, in_df, groups = group_name,missing = 'drop').fit()\n",
    "    return model\n",
    "\n",
    "def run_gee(in_df,group_name,formula,cov_structure, resp_family):\n",
    "    \"\"\" \n",
    "    Run statsmodels GEE.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pandas DataFrame\n",
    "        input dataframe\n",
    "    group_name: str\n",
    "        column to group by\n",
    "    formula:    str\n",
    "        patsy formula\n",
    "    cov_structure:  sm covariance structure\n",
    "        covariance structure (e.g. sm.cov_struct.Independence())\n",
    "    resp_family:    sm family (e.g. sm.families.Tweedie())\n",
    "        mean response structure distribution\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    model = smf.gee(formula,group_name, in_df, cov_struct = cov_structure, family = resp_family,missing = 'drop').fit()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = 1\n",
    "if home:\n",
    "    infiledir = r\"C:\\Users\\Luzia T\\UCL\\WorkingFromHome\\Possible_online_studies\\NLP_expressive_writing\\analysis\\Processed_2\"\n",
    "    writing_dir = r\"C:\\Users\\Luzia T\\UCL\\WorkingFromHome\\Possible_online_studies\\NLP_expressive_writing\\analysis\\writing_data\\statements\"\n",
    "else:\n",
    "    infiledir = r\"P:\\EW_analysis\\analysis\\Processed_2\"\n",
    "    writing_dir = r\"P:\\EW_analysis\\analysis\\writing\\writing_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_df = pd.read_csv(os.path.join(writing_dir, 'writing_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing raw texts ...\n",
      "Preprocessing raw texts. Done!\n",
      "Getting sentiment scores...\n",
      "Done!\n",
      "\n",
      "P value (word_count) is 0.08764873320117955.\n",
      "No significant between group differences on word_count.\n",
      "\n",
      "P value (word_count_raw) is 0.0029349359121382448.\n",
      "Conditions differ significantly on word_count_raw.\n",
      "Posthoc (word_count_raw) is:\n",
      "          1         2         3\n",
      "1  1.000000  0.083669  0.002315\n",
      "2  0.083669  1.000000  0.748153\n",
      "3  0.002315  0.748153  1.000000.\n",
      "The key is 1 = CTR, 2 = EW, 3 = EWRE\n",
      "\n",
      "P value (polarity) is 0.03688590519079081.\n",
      "Conditions differ significantly on polarity.\n",
      "Posthoc (polarity) is:\n",
      "          1        2         3\n",
      "1  1.000000  1.00000  0.124654\n",
      "2  1.000000  1.00000  0.053490\n",
      "3  0.124654  0.05349  1.000000.\n",
      "The key is 1 = CTR, 2 = EW, 3 = EWRE\n",
      "\n",
      "P value (subjectivity) is 1.3978564535849075e-31.\n",
      "Conditions differ significantly on subjectivity.\n",
      "Posthoc (subjectivity) is:\n",
      "              1             2             3\n",
      "1  1.000000e+00  1.510864e-25  1.931276e-23\n",
      "2  1.510864e-25  1.000000e+00  1.000000e+00\n",
      "3  1.931276e-23  1.000000e+00  1.000000e+00.\n",
      "The key is 1 = CTR, 2 = EW, 3 = EWRE\n"
     ]
    }
   ],
   "source": [
    "sentences,tokens = preprocess(writing_df.writing)\n",
    "writing_df = writing_df.assign(\n",
    "                                writing_tokens=tokens,\n",
    "                                writing_sents = sentences\n",
    "                                )\n",
    "writing_df = writing_df.assign(\n",
    "                                word_count = writing_df.writing_tokens.apply(len)\n",
    "                                )\n",
    "writing_df = writing_df.assign(\n",
    "                                word_count_raw =\n",
    "                                writing_df.writing.apply(lambda x: len(x.split()))\n",
    "                                )\n",
    "writing_df = get_sentiment(writing_df,'writing')\n",
    "for val in ['word_count','word_count_raw','polarity','subjectivity']:\n",
    "        # check whether word count is significantly different between conditions:\n",
    "        _, pval, posthoc, key = kruskal_wallis_func(\n",
    "                                        writing_df,'Group', val\n",
    "                                        )\n",
    "        print(f\"\\nP value ({val}) is {pval}.\")\n",
    "        if pval<0.05:\n",
    "                print(f\"Conditions differ significantly on {val}.\")\n",
    "                print(f\"Posthoc ({val}) is:\\n{posthoc}.\")\n",
    "                print(f\"The key is 1 = {key[0]}, 2 = {key[1]}, 3 = {key[2]}\")\n",
    "        else:\n",
    "                print(f\"No significant between group differences on {val}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) There should be no differences between EW & EWRE on D1.\n",
    "(ii) D2: effects/responsibility\n",
    "(iii) D3: different angles/perspectives\n",
    "(iv) D4: learnt/gained/future perspectives\n",
    "Change in most frequent words?\n",
    "Change in polarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day-by-day changes, comparing EW and EWRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luzia T\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#model_polarity = run_mixedlm(writing_df[writing_df.Group.isin(['EW','EWRE'])], 'id', \"polarity ~ C(day) * C(Group)\", re_intercept = \"~day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_polarity_gee = run_gee(writing_df[writing_df.Group.isin(['EW','EWRE'])], \"id\", \"polarity ~ C(day) * C(Group)\", cov_structure = sm.cov_struct.Independence(), resp_family = sm.families.Gaussian())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>GEE</td>            <td>AIC:</td>        <td>-720.9234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>Link Function:</td>        <td>identity</td>          <td>BIC:</td>       <td>-1804.5687</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>polarity</td>     <td>Log-Likelihood:</td>   <td>368.46</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2022-01-02 20:32</td>    <td>LL-Null:</td>       <td>361.83</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>321</td>          <td>Deviance:</td>      <td>1.8924</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>7</td>         <td>Pearson chi2:</td>     <td>1.89</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>313</td>           <td>Scale:</td>       <td>0.0060460</td>\n",
       "</tr>\n",
       "<tr>\n",
       "        <td>Method:</td>             <td>IRLS</td>              <td></td>              <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                <td></td>                <th>Coef.</th>  <th>Std.Err.</th>    <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                    <td>0.0400</td>   <td>0.0102</td>  <td>3.9338</td>  <td>0.0001</td> <td>0.0201</td>  <td>0.0600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(day)[T.2]</th>                  <td>0.0060</td>   <td>0.0156</td>  <td>0.3843</td>  <td>0.7008</td> <td>-0.0245</td> <td>0.0365</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(day)[T.3]</th>                  <td>-0.0038</td>  <td>0.0165</td>  <td>-0.2295</td> <td>0.8184</td> <td>-0.0361</td> <td>0.0286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(day)[T.4]</th>                  <td>-0.0058</td>  <td>0.0156</td>  <td>-0.3705</td> <td>0.7110</td> <td>-0.0363</td> <td>0.0247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Group)[T.EWRE]</th>             <td>0.0006</td>   <td>0.0140</td>  <td>0.0402</td>  <td>0.9679</td> <td>-0.0269</td> <td>0.0280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(day)[T.2]:C(Group)[T.EWRE]</th> <td>0.0087</td>   <td>0.0214</td>  <td>0.4072</td>  <td>0.6839</td> <td>-0.0333</td> <td>0.0508</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(day)[T.3]:C(Group)[T.EWRE]</th> <td>0.0307</td>   <td>0.0212</td>  <td>1.4484</td>  <td>0.1475</td> <td>-0.0108</td> <td>0.0723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(day)[T.4]:C(Group)[T.EWRE]</th> <td>0.0471</td>   <td>0.0223</td>  <td>2.1109</td>  <td>0.0348</td> <td>0.0034</td>  <td>0.0908</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                               Results: GEE\n",
       "===========================================================================\n",
       "Model:                  GEE                  AIC:                -720.9234 \n",
       "Link Function:          identity             BIC:                -1804.5687\n",
       "Dependent Variable:     polarity             Log-Likelihood:     368.46    \n",
       "Date:                   2022-01-02 20:32     LL-Null:            361.83    \n",
       "No. Observations:       321                  Deviance:           1.8924    \n",
       "Df Model:               7                    Pearson chi2:       1.89      \n",
       "Df Residuals:           313                  Scale:              0.0060460 \n",
       "Method:                 IRLS                                               \n",
       "---------------------------------------------------------------------------\n",
       "                              Coef.  Std.Err.    z    P>|z|   [0.025 0.975]\n",
       "---------------------------------------------------------------------------\n",
       "Intercept                     0.0400   0.0102  3.9338 0.0001  0.0201 0.0600\n",
       "C(day)[T.2]                   0.0060   0.0156  0.3843 0.7008 -0.0245 0.0365\n",
       "C(day)[T.3]                  -0.0038   0.0165 -0.2295 0.8184 -0.0361 0.0286\n",
       "C(day)[T.4]                  -0.0058   0.0156 -0.3705 0.7110 -0.0363 0.0247\n",
       "C(Group)[T.EWRE]              0.0006   0.0140  0.0402 0.9679 -0.0269 0.0280\n",
       "C(day)[T.2]:C(Group)[T.EWRE]  0.0087   0.0214  0.4072 0.6839 -0.0333 0.0508\n",
       "C(day)[T.3]:C(Group)[T.EWRE]  0.0307   0.0212  1.4484 0.1475 -0.0108 0.0723\n",
       "C(day)[T.4]:C(Group)[T.EWRE]  0.0471   0.0223  2.1109 0.0348  0.0034 0.0908\n",
       "===========================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_polarity_gee.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>writing_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[week, pandem, plan, time, famili, health, iss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[distress, situat, situat, time, pandem, peopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[death, nan, ill, thought, etc, time, everyth,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[today, impact, pandem, live, way, peopl, thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[begin, year, news, realiti, situat, famili, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>[week, week, term, schedul, monday, tutori, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>[week, schedul, plan, morn, lay, till, plan, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>[teeth, face, face, cream, sunscreen, cream, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>[today, email, home, worker, date, depart, hom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>[rest, week, workout, lesson, wednesday, morn,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>481 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        writing_tokens\n",
       "0    [week, pandem, plan, time, famili, health, iss...\n",
       "1    [distress, situat, situat, time, pandem, peopl...\n",
       "2    [death, nan, ill, thought, etc, time, everyth,...\n",
       "3    [today, impact, pandem, live, way, peopl, thou...\n",
       "4    [begin, year, news, realiti, situat, famili, c...\n",
       "..                                                 ...\n",
       "476  [week, week, term, schedul, monday, tutori, ho...\n",
       "477  [week, schedul, plan, morn, lay, till, plan, r...\n",
       "478  [teeth, face, face, cream, sunscreen, cream, p...\n",
       "479  [today, email, home, worker, date, depart, hom...\n",
       "480  [rest, week, workout, lesson, wednesday, morn,...\n",
       "\n",
       "[481 rows x 1 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writing_df.filter(like = 'token',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "day_1_df = writing_df.loc[writing_df.day==1, ['id','Group','writing','writing_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_1_df['Counts'] = day_1_df.writing_tokens.apply(FreqDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_1_df.Counts[0].most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "def get_top_words(day_col, num_day, group_col, group_name, in_df, token_col_name):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    token_list = [item for sublist\n",
    "                in [*in_df.loc[\n",
    "                (in_df[group_col] == group_name) & (in_df[day_col] == num_day),token_col_name]\n",
    "                ]\n",
    "                for item in sublist]\n",
    "    freqs = FreqDist(token_list)\n",
    "    common_tups = freqs.most_common()\n",
    "    common_words = list(zip(*common_tups))[0]\n",
    "    common_vals = list(zip(*common_tups))[1]\n",
    "\n",
    "    return common_words, common_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_ew_day1, vals_ew_day1 = get_top_words('day', 1, 'Group', 'EW', writing_df, 'writing_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,51),vals_ew_day1[:50])\n",
    "plt.xticks(ticks = np.arange(1,51),labels = words_ew_day1[:50])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ec994d0b4d2f6c713992b16f2c88007ac3578bc75aec34272bb3a023418f6ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
