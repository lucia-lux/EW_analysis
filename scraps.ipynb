{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pkg_resources\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "if sym_spell.word_count:\n",
    "    pass\n",
    "else:\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# normalization\n",
    "def func_norm(s):\n",
    "    \"\"\"\n",
    "    Perform some basic normalisation operations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s:  str\n",
    "        text to operate on\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Normalised string\n",
    "    \n",
    "    \"\"\"\n",
    "    s = s.lower() # lower case\n",
    "    # letter repetition (>2)\n",
    "    s  = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n",
    "    # non word repetition\n",
    "    s = s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n",
    "    # noise text\n",
    "    s = re.sub(r' ing ', ' ', s)\n",
    "    # phrase repetition\n",
    "    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "def func_punc(w_list):\n",
    "    \"\"\"\n",
    "    Remove non-alphabet characters. Includes punctuation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        list of tokens to be processed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        list without non-alphabet characters\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word.isalpha()]\n",
    "\n",
    "def func_stopf(w_list):\n",
    "    \"\"\"\n",
    "    Remove stop words\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        list of tokens to be processed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        list without stop words\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    w_list  = [f for f in w_list if f not in stop_words]\n",
    "    return w_list\n",
    "\n",
    "# stemming\n",
    "pstem = PorterStemmer()\n",
    "\n",
    "def func_stem(w_list):\n",
    "    \"\"\"\n",
    "    stem word list\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        word list for stemming\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        stemmed word list \n",
    "    \"\"\"\n",
    "    sw_list = [pstem.stem(w) for w in w_list]\n",
    "    return sw_list\n",
    "\n",
    "# selecting nouns\n",
    "def func_noun(w_list):\n",
    "    \"\"\"\n",
    "    in: word list to be processed\n",
    "    out: w_list with only nouns selected\n",
    "    \"\"\"\n",
    "    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n",
    "\n",
    "def func_verb(w_list):\n",
    "    \"\"\" \n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        list of tokens to be processed\n",
    "    Returns\n",
    "    -------\n",
    "        list of verbs only\n",
    "    \"\"\"\n",
    "    return [word for (word,pos) in nltk.pos_tag(w_list) if pos[:2] == 'VB']\n",
    "\n",
    "def func_adjective(w_list):\n",
    "    \"\"\" \n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        list of tokens to be processed\n",
    "    Returns\n",
    "    -------\n",
    "        list of verbs only\n",
    "    \"\"\"\n",
    "    return [word for (word,pos) in nltk.pos_tag(w_list) if pos[:2] == 'JJ']\n",
    "\n",
    "def func_inf_words(w_list):\n",
    "    \"\"\" \n",
    "    Retain verbs, adjectives and nouns only\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        list of words to be processed\n",
    "    Returns\n",
    "    -------\n",
    "    list of nouns, adjectives and verbs only\n",
    "    \"\"\"\n",
    "    return [word for (word,pos) in nltk.pos_tag(w_list) if 'VB' in pos or 'JJ' in pos or 'NN' in pos]\n",
    "\n",
    "# spell checker/typo correction\n",
    "def func_spell(w_list):\n",
    "    \"\"\"\n",
    "    in: word list to be processed\n",
    "    out: w_list with typo fixed by symspell. words with no match up will be dropped\n",
    "    \"\"\"\n",
    "    w_list_fixed = []\n",
    "    for word in w_list:\n",
    "        if any(map(word.__contains__, ['covid','lockdown'])):\n",
    "            w_list_fixed.append(word)\n",
    "        else:\n",
    "            suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n",
    "            if suggestions:\n",
    "                w_list_fixed.append(suggestions[0].term)\n",
    "            else:\n",
    "                pass\n",
    "    return w_list_fixed\n",
    "\n",
    "def get_pos_tag(tag):\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def func_lemmatize(word_list):\n",
    "    \"\"\"\n",
    "    Lemmatize word list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word_list:  list\n",
    "        words to process\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Lemmatized word list.\n",
    "\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_list = nltk.pos_tag(word_list)\n",
    "    words_lemmatized = [lemmatizer.lemmatize(word,get_pos_tag(tag))\n",
    "                        for (word,tag) in word_list]\n",
    "    return words_lemmatized\n",
    "\n",
    "def preprocess_sent(rw):\n",
    "    \"\"\"\n",
    "    Get sentence level preprocessed data from raw texts\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        rw: str\n",
    "            sentence to be processed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        sentence level pre-processed text\n",
    "    \"\"\"\n",
    "    s = func_norm(rw)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_word(s):\n",
    "    \"\"\"\n",
    "    Get word level preprocessed data from preprocessed sentences.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s:  str\n",
    "        sentence to be processed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        word level pre-processed text\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    w_list = word_tokenize(s)\n",
    "    w_list = func_punc(w_list)\n",
    "   # w_list = func_inf_words(w_list)\n",
    "    w_list = func_spell(w_list)\n",
    "    w_list = func_lemmatize(w_list)\n",
    "    w_list = func_stopf(w_list)\n",
    "\n",
    "    return w_list\n",
    "\n",
    "def preprocess(docs):\n",
    "    \"\"\"\n",
    "    Preprocess the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs: list\n",
    "        list of documents to be preprocessed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Preprocessed sentences, tokens\n",
    "    \"\"\"\n",
    "    print('Preprocessing raw texts ...')\n",
    "    #n_docs = len(docs)\n",
    "    sentences = []  # sentence level preprocessed\n",
    "    token_lists = []  # word level preprocessed\n",
    "    #samp = np.random.choice(n_docs)\n",
    "    for i in range(0, len(docs)):\n",
    "        sentence = preprocess_sent(docs.iloc[i])\n",
    "        token_list = preprocess_word(sentence)\n",
    "        if token_list:\n",
    "            sentences.append(sentence)\n",
    "            token_lists.append(token_list)\n",
    "        print('{} %'.format(str(np.round((i + 1) / len(writing_df.writing) * 100, 2))), end='\\r')\n",
    "    print('Preprocessing raw texts. Done!')\n",
    "    return sentences, token_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikit_posthocs as sp\n",
    "from scipy import stats\n",
    "from textblob import TextBlob\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "def kruskal_wallis_func(in_df, group_col, test_col):\n",
    "    \"\"\"\n",
    "    Kruskal Wallis test and\n",
    "    post-hoc Dunn's.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd DataFrame\n",
    "        input dataframe\n",
    "    group_col:  str\n",
    "        name of group column\n",
    "    test_col:   str\n",
    "        name of column containing\n",
    "        relevant values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Statistic, pvalue\n",
    "    \"\"\"\n",
    "    data = in_df.pivot(columns = group_col, values = test_col)\n",
    "    if len(in_df[group_col].unique())>2:\n",
    "            statistic,pval = stats.kruskal(data.iloc[:,0],data.iloc[:,1],\n",
    "            data.iloc[:,2],nan_policy = 'omit')\n",
    "            posthoc = sp.posthoc_dunn(\n",
    "                        [data.iloc[:,0].dropna(),data.iloc[:,1].dropna(),data.iloc[:,2].dropna()],\n",
    "                        p_adjust = 'bonferroni'\n",
    "                        )\n",
    "            key = [data.columns[0],data.columns[1],data.columns[2]]\n",
    "    else:\n",
    "        statistic,pval = stats.kruskal(data.iloc[:,0],data.iloc[:,1],\n",
    "        nan_policy = 'omit')\n",
    "        posthoc = None\n",
    "        key = None\n",
    "    return statistic,pval, posthoc, key\n",
    "\n",
    "\n",
    "def get_sentiment(in_df, in_col):\n",
    "    \"\"\"\n",
    "    Get subjectivity\n",
    "    and polarity scores.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd DataFrame\n",
    "        DataFrame to operate on\n",
    "    in_col: str\n",
    "        column holding text data\n",
    "        to operate on\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Input DataFrame with\n",
    "    subjectivity/polarity columns\n",
    "    added.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Getting sentiment scores...\")\n",
    "    in_df = in_df.assign(\n",
    "                        polarity = in_df[in_col].astype('str').apply(\n",
    "                        [lambda x: TextBlob(x).sentiment.polarity]),\n",
    "                        subjectivity = in_df[in_col].astype('str').apply(\n",
    "                        [lambda x: TextBlob(x).sentiment.subjectivity])\n",
    "                        )\n",
    "    print(\"Done!\")\n",
    "    return in_df\n",
    "\n",
    "def run_mixedlm(in_df,group_name,formula, re_intercept):\n",
    "    \"\"\" \n",
    "    Run statsmodels LMEM.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pandas DataFrame\n",
    "        input dataframe\n",
    "    group_name: str\n",
    "        column to group by\n",
    "    formula:    str\n",
    "        patsy formula\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MixedLMResults instance\n",
    "    \"\"\"\n",
    "    if re_intercept:\n",
    "        model = smf.mixedlm(\n",
    "                            formula, in_df, groups = group_name,re_formula = re_intercept, missing = 'drop'\n",
    "                            ).fit()\n",
    "    else:\n",
    "        model = smf.mixedlm(formula, in_df, groups = group_name,missing = 'drop').fit()\n",
    "    return model\n",
    "\n",
    "def run_gee(in_df,group_name,formula,cov_structure, resp_family):\n",
    "    \"\"\" \n",
    "    Run statsmodels GEE.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pandas DataFrame\n",
    "        input dataframe\n",
    "    group_name: str\n",
    "        column to group by\n",
    "    formula:    str\n",
    "        patsy formula\n",
    "    cov_structure:  sm covariance structure\n",
    "        covariance structure (e.g. sm.cov_struct.Independence())\n",
    "    resp_family:    sm family (e.g. sm.families.Tweedie())\n",
    "        mean response structure distribution\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    model = smf.gee(formula,group_name, in_df, cov_struct = cov_structure, family = resp_family,missing = 'drop').fit()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def flatten_list(list_of_lists):\n",
    "    \"\"\"\n",
    "    Flatten list of lists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_of_lists:  list\n",
    "        list of lists to flatten\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Flattened list.\n",
    "    \"\"\"\n",
    "    return [item for sub_list in list_of_lists for item in sub_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn import feature_extraction, model_selection, pipeline, metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "class Word_Analyzer:\n",
    "    \"\"\"\n",
    "    Class for word frequency based analysis.\n",
    "    \"\"\"\n",
    "    def __init__(self, day_col_name, group_col_name, token_col_name):\n",
    "        self.day_col = day_col_name\n",
    "        self.group_col = group_col_name\n",
    "        self.token_col = token_col_name\n",
    "\n",
    "    def get_pos_tag(self, tag):\n",
    "        \"\"\"\n",
    "        Get wordnet pos tag.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tag:    str\n",
    "        POS tag (from nltk pos_tag output tuple)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        wordnet pos tag\n",
    "        can be passed to nltk lemmatizer.\n",
    "        \"\"\"\n",
    "\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "    \n",
    "    def func_lemmatize(self, word_list):\n",
    "        \"\"\"\n",
    "        Lemmatize word list.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        word_list:  list\n",
    "            words to process\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Lemmatized word list.\n",
    "\n",
    "        \"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        word_list = nltk.pos_tag(word_list)\n",
    "        words_lemmatized = [lemmatizer.lemmatize(word,get_pos_tag(tag))\n",
    "                            for (word,tag) in word_list]\n",
    "        return words_lemmatized\n",
    "    \n",
    "    def get_top_words(self, num_day, group_name, in_df, **pos_tag_type):\n",
    "        \"\"\" \n",
    "        Get an ordered list of words in document.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_day:    int\n",
    "            Day of writing (1, 2, 3 or 4)\n",
    "        group_name: str\n",
    "            The group to process (EW, EWRE or CTR)\n",
    "        in_df:  pd DataFrame\n",
    "            input dataframe containing rel data\n",
    "        **pos_tag_type: list of str\n",
    "            If processing only nouns/verbs/adjectives\n",
    "            pass tag to function using kwargs.\n",
    "            For adjectives, use:\n",
    "            'JJ'\n",
    "            For verbs, use:\n",
    "            'VB'\n",
    "            For nouns, use:\n",
    "            'NN'\n",
    "            If all of the above, pass list:\n",
    "            ['NN','JJ','VB']\n",
    "            as kwarg.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of words, list of vals\n",
    "        words = words ordered from most frequent to rare\n",
    "        vals = corresponding frequency\n",
    "        \"\"\"\n",
    "        token_list = [\n",
    "                    item for sublist in\n",
    "                    [*in_df.loc[\n",
    "                    (in_df[self.group_col] == group_name) &\n",
    "                    (in_df[self.day_col] == num_day),\n",
    "                    self.token_col]]\n",
    "                    for item in sublist\n",
    "                    ]\n",
    "\n",
    "    \n",
    "        if pos_tag_type:\n",
    "            selected_list = []\n",
    "            for tag_type in pos_tag_type.values():\n",
    "                w_list = [\n",
    "                        word for (word,pos) \n",
    "                        in nltk.pos_tag(token_list)\n",
    "                        for tag in tag_type\n",
    "                        if pos[:2] == tag\n",
    "                        ]\n",
    "                selected_list.extend(w_list)\n",
    "        else:\n",
    "            selected_list = token_list\n",
    "        freqs = FreqDist(selected_list)\n",
    "        common_tups = freqs.most_common()\n",
    "        self.common_words = list(zip(*common_tups))[0]\n",
    "        self.common_vals = list(zip(*common_tups))[1]\n",
    "        return self.common_words, self.common_vals\n",
    "    \n",
    "    def func_top_words(self, in_df, pos_tags, visualize):\n",
    "        \"\"\"\n",
    "        Put top 50 words in dataframe,\n",
    "        with option to visualize using barplots.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_df:  pd DataFrame\n",
    "        input dataframe\n",
    "        pos_tags:   list\n",
    "            list of pos tags to use\n",
    "            can be VB, JJ, NN or\n",
    "            any combination (or all) of these\n",
    "        visualize:  int\n",
    "        1 if visualization is needed\n",
    "        0 otherwise\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Datframe of 50 top words\n",
    "        and their frequencies for \n",
    "        all days and conditions.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        top_50_words = []\n",
    "        top_50_vals = []\n",
    "        condition = []\n",
    "        days = []\n",
    "\n",
    "        for group_name in in_df[self.group_col].unique():\n",
    "            for day in in_df[self.day_col].unique():\n",
    "                words,vals= self.get_top_words(day, group_name, in_df, pos_tags = pos_tags)\n",
    "                top_50_words.append(list(words[:50]))\n",
    "                top_50_vals.append(list(vals[:50]))\n",
    "                condition.append(np.repeat(group_name,50))\n",
    "                days.append(np.repeat(day,50))\n",
    "\n",
    "        data = {\n",
    "            'words': flatten_list(top_50_words),\n",
    "            'vals': flatten_list(top_50_vals),\n",
    "            'day': flatten_list(days),\n",
    "            'group': flatten_list(condition)\n",
    "            }\n",
    "        self.most_common_words_df = pd.DataFrame(data)\n",
    "\n",
    "        if visualize == 1:\n",
    "            for num_day in self.most_common_words_df.day.unique():\n",
    "                fig,axes = plt.subplots(3,1,figsize = (30,15),sharey = True)\n",
    "                for i, group_name in enumerate(self.most_common_words_df.group.unique()):\n",
    "                    data = self.most_common_words_df.loc[\n",
    "                                                    (self.most_common_words_df.day==num_day) &\n",
    "                                                    (self.most_common_words_df.group == group_name),\n",
    "                                                    ['words','vals']\n",
    "                                                    ]\n",
    "                    sns.barplot(ax=axes[i], x=data.words, y=data.vals)\n",
    "                    axes[i].set_title(f'Condition: {group_name}, Day: {num_day}')\n",
    "\n",
    "        return self.most_common_words_df\n",
    "    \n",
    "    def print_top10(self, vectorizer, clf):\n",
    "        \"\"\"\n",
    "        Prints features with the highest coefficient values,\n",
    "        per class\n",
    "\n",
    "        \"\"\"\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "        for i, class_label in enumerate(clf.classes_):\n",
    "            top10 = np.argsort(clf.coef_[i])[-15:]\n",
    "            print(\"%s: %s\" % (class_label,\n",
    "                \" \".join(feature_names[j] for j in top10)))\n",
    "\n",
    "    def tf_idf_scores(self, in_df, writing_col,*cleaned):\n",
    "        \"\"\"\n",
    "        Classify statements using Linear SVC\n",
    "        and print top 10 distinguishing features\n",
    "\n",
    "        in_df:  pd DataFrame\n",
    "            input dataframe\n",
    "        \n",
    "        writing_col:    str\n",
    "            name of column containing written statements\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dataframe containing predictions\n",
    "        and actual class labels\n",
    "        for holdout test set\n",
    "        \"\"\"\n",
    "        if cleaned:\n",
    "            in_df['writing_cleaned'] = in_df[self.token_col].apply(lambda x: ' '.join(x))\n",
    "            writing_col = 'writing_cleaned'\n",
    "        \n",
    "        self.wrdf_train,self.wrdf_test = model_selection.train_test_split(in_df.loc[:,[writing_col, self.group_col]],\n",
    "                                                                test_size = 0.3,random_state = 35,\n",
    "                                                                stratify = in_df[self.group_col])\n",
    "        y_train = self.wrdf_train[self.group_col]\n",
    "        y_test = self.wrdf_test[self.group_col]\n",
    "        vectorizer_tf_idf = feature_extraction.text.TfidfVectorizer(sublinear_tf = True)\n",
    "        clf = LinearSVC(C=1.0, class_weight=\"balanced\")\n",
    "        tf_idf = pipeline.Pipeline([('tfidf', vectorizer_tf_idf),(\"classifier\", clf)])\n",
    "        tf_idf.fit(self.wrdf_train[writing_col], y_train)\n",
    "        predicted = tf_idf.predict(self.wrdf_test[writing_col])\n",
    "\n",
    "        self.res_df = pd.DataFrame({'actual': y_test.values, 'predicted': predicted})\n",
    "        self.print_top10(vectorizer_tf_idf,clf)\n",
    "\n",
    "        return self.res_df,clf\n",
    "\n",
    "\n",
    "    \n",
    "    def tf_idf_features(self, in_df, writing_col,*cleaned):\n",
    "        \"\"\"\n",
    "        get TF-IDF features\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus: sequence of items of type str\n",
    "            text corpus to be processed\n",
    "\n",
    "        kwargs:\n",
    "        num_day:    int\n",
    "         Integer denoting day of writing\n",
    "        group_name: str\n",
    "            String denoting writing condition\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List of features\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "                            \n",
    "        if cleaned:\n",
    "            corpus = in_df[self.token_col].apply(lambda x: ' '.join(x))\n",
    "        else:\n",
    "            corpus = in_df[writing_col]\n",
    "        vectorizer_tf_idf = feature_extraction.text.TfidfVectorizer(sublinear_tf = True)\n",
    "        features = vectorizer_tf_idf.fit_transform(corpus)\n",
    "        feats_df = pd.DataFrame(features[0].T.todense(), index=vectorizer_tf_idf.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "        self.feats_df = feats_df.sort_values('TF-IDF', ascending=False)\n",
    "\n",
    "        return self.feats_df\n",
    "    \n",
    "    def plot_confusion_matrix(self):\n",
    "        \"\"\"\n",
    "        Plot the confusion matrix\n",
    "        for classifier results.\n",
    "        \"\"\"\n",
    "        # Plot the confusion matrix.\n",
    "        y_test = self.res_df['actual']\n",
    "        classes = np.unique(y_test)\n",
    "        #y_test_array = pd.get_dummies(y_test, drop_first=False).values    \n",
    "        cm = metrics.confusion_matrix(y_test, self.res_df['predicted'])\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, cbar=False)\n",
    "        ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels= classes, yticklabels=classes, title=\"Confusion matrix\")\n",
    "        plt.yticks(rotation=0) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = 0\n",
    "if home:\n",
    "    infiledir = r\"C:\\Users\\Luzia T\\UCL\\WorkingFromHome\\Possible_online_studies\\NLP_expressive_writing\\analysis\\Processed_2\"\n",
    "    writing_dir = r\"C:\\Users\\Luzia T\\UCL\\WorkingFromHome\\Possible_online_studies\\NLP_expressive_writing\\analysis\\writing_data\\statements\"\n",
    "else:\n",
    "    infiledir = r\"P:\\EW_analysis\\analysis\\Processed_2\"\n",
    "    writing_dir = r\"P:\\EW_analysis\\analysis\\writing\\writing_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_df = pd.read_csv(os.path.join(writing_dir, 'writing_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing raw texts ...\n",
      "Preprocessing raw texts. Done!\n",
      "Getting sentiment scores...\n",
      "Done!\n",
      "\n",
      "P value (word_count) is 0.00972972280113964.\n",
      "Conditions differ significantly on word_count.\n",
      "Posthoc (word_count) is:\n",
      "          1         2         3\n",
      "1  1.000000  0.283874  0.007122\n",
      "2  0.283874  1.000000  0.523870\n",
      "3  0.007122  0.523870  1.000000.\n",
      "The key is 1 = CTR, 2 = EW, 3 = EWRE\n",
      "\n",
      "P value (word_count_raw) is 0.0029349359121382448.\n",
      "Conditions differ significantly on word_count_raw.\n",
      "Posthoc (word_count_raw) is:\n",
      "          1         2         3\n",
      "1  1.000000  0.083669  0.002315\n",
      "2  0.083669  1.000000  0.748153\n",
      "3  0.002315  0.748153  1.000000.\n",
      "The key is 1 = CTR, 2 = EW, 3 = EWRE\n",
      "\n",
      "P value (polarity) is 0.03688590519079081.\n",
      "Conditions differ significantly on polarity.\n",
      "Posthoc (polarity) is:\n",
      "          1        2         3\n",
      "1  1.000000  1.00000  0.124654\n",
      "2  1.000000  1.00000  0.053490\n",
      "3  0.124654  0.05349  1.000000.\n",
      "The key is 1 = CTR, 2 = EW, 3 = EWRE\n",
      "\n",
      "P value (subjectivity) is 1.3978564535849075e-31.\n",
      "Conditions differ significantly on subjectivity.\n",
      "Posthoc (subjectivity) is:\n",
      "              1             2             3\n",
      "1  1.000000e+00  1.510864e-25  1.931276e-23\n",
      "2  1.510864e-25  1.000000e+00  1.000000e+00\n",
      "3  1.931276e-23  1.000000e+00  1.000000e+00.\n",
      "The key is 1 = CTR, 2 = EW, 3 = EWRE\n"
     ]
    }
   ],
   "source": [
    "sentences,tokens = preprocess(writing_df.writing)\n",
    "writing_df = writing_df.assign(\n",
    "                                writing_tokens=tokens,\n",
    "                                writing_sents = sentences\n",
    "                                )\n",
    "writing_df = writing_df.assign(\n",
    "                                word_count = writing_df.writing_tokens.apply(len)\n",
    "                                )\n",
    "writing_df = writing_df.assign(\n",
    "                                word_count_raw =\n",
    "                                writing_df.writing.apply(lambda x: len(x.split()))\n",
    "                                )\n",
    "writing_df = get_sentiment(writing_df,'writing')\n",
    "for val in ['word_count','word_count_raw','polarity','subjectivity']:\n",
    "        # check whether word count is significantly different between conditions:\n",
    "        _, pval, posthoc, key = kruskal_wallis_func(\n",
    "                                        writing_df,'Group', val\n",
    "                                        )\n",
    "        print(f\"\\nP value ({val}) is {pval}.\")\n",
    "        if pval<0.05:\n",
    "                print(f\"Conditions differ significantly on {val}.\")\n",
    "                print(f\"Posthoc ({val}) is:\\n{posthoc}.\")\n",
    "                print(f\"The key is 1 = {key[0]}, 2 = {key[1]}, 3 = {key[2]}\")\n",
    "        else:\n",
    "                print(f\"No significant between group differences on {val}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-depth analysis of between-condition differences (EW/EWRE)\n",
    "(i) There should be no differences between EW & EWRE on D1.\n",
    "(ii) D2: effects/responsibility\n",
    "(iii) D3: different angles/perspectives\n",
    "(iv) D4: learnt/gained/future perspectives\n",
    "Change in most frequent words?\n",
    "Change in polarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day-by-day changes, comparing EW and EWRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Results: GEE\n",
      "===========================================================================\n",
      "Model:                  GEE                  AIC:                -720.9234 \n",
      "Link Function:          identity             BIC:                -1804.5687\n",
      "Dependent Variable:     polarity             Log-Likelihood:     368.46    \n",
      "Date:                   2022-01-04 18:00     LL-Null:            361.83    \n",
      "No. Observations:       321                  Deviance:           1.8924    \n",
      "Df Model:               7                    Pearson chi2:       1.89      \n",
      "Df Residuals:           313                  Scale:              0.0060460 \n",
      "Method:                 IRLS                                               \n",
      "---------------------------------------------------------------------------\n",
      "                              Coef.  Std.Err.    z    P>|z|   [0.025 0.975]\n",
      "---------------------------------------------------------------------------\n",
      "Intercept                     0.0400   0.0102  3.9338 0.0001  0.0201 0.0600\n",
      "C(day)[T.2]                   0.0060   0.0156  0.3843 0.7008 -0.0245 0.0365\n",
      "C(day)[T.3]                  -0.0038   0.0165 -0.2295 0.8184 -0.0361 0.0286\n",
      "C(day)[T.4]                  -0.0058   0.0156 -0.3705 0.7110 -0.0363 0.0247\n",
      "C(Group)[T.EWRE]              0.0006   0.0140  0.0402 0.9679 -0.0269 0.0280\n",
      "C(day)[T.2]:C(Group)[T.EWRE]  0.0087   0.0214  0.4072 0.6839 -0.0333 0.0508\n",
      "C(day)[T.3]:C(Group)[T.EWRE]  0.0307   0.0212  1.4484 0.1475 -0.0108 0.0723\n",
      "C(day)[T.4]:C(Group)[T.EWRE]  0.0471   0.0223  2.1109 0.0348  0.0034 0.0908\n",
      "===========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_polarity_gee = run_gee(\n",
    "                            writing_df[writing_df.Group.isin(['EW','EWRE'])],\n",
    "                            \"id\", \"polarity ~ C(day) * C(Group)\",\n",
    "                            cov_structure = sm.cov_struct.Independence(),\n",
    "                            resp_family = sm.families.Gaussian()\n",
    "                            )\n",
    "\n",
    "print(model_polarity_gee.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa = Word_Analyzer('day','Group','writing_tokens')\n",
    "top_adj_df = wa.func_top_words(writing_df, ['JJ'],0)\n",
    "top_vbs_df = wa.func_top_words(writing_df,['VB'], 0)\n",
    "top_nns_df = wa.func_top_words(writing_df, ['NN'],0)\n",
    "top_words_df = wa.func_top_words(writing_df, ['VB','JJ','NN'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTR: morning coffee wash book finish around walk minute lunch plan wake dinner bed breakfast watch\n",
      "EW: apart stomach whether effect friendship contact tell university miss less country never etc everyone pandemic\n",
      "EWRE: uncle learn want happy felt career feel guess covid date experience find lockdown end especially\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_df,clf = wa.tf_idf_scores(writing_df,'writing',1)\n",
    "wa.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excite      0.149507\n",
      "elderly     0.149507\n",
      "skype       0.149507\n",
      "reassure    0.149507\n",
      "hopeful     0.144282\n",
      "              ...   \n",
      "wine        0.000000\n",
      "wing        0.000000\n",
      "winter      0.000000\n",
      "wipe        0.000000\n",
      "zoom        0.000000\n",
      "Name: TF-IDF, Length: 7077, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "tf_idf_features = wa.tf_idf_features(writing_df,'writing',1)\n",
    "print(tf_idf_features['TF-IDF'].sort_values(ascending = False)[10:])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ec994d0b4d2f6c713992b16f2c88007ac3578bc75aec34272bb3a023418f6ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
