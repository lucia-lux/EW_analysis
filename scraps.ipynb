{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pkg_resources\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "if sym_spell.word_count:\n",
    "    pass\n",
    "else:\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# normalization\n",
    "def func_norm(s):\n",
    "    \"\"\"\n",
    "    Perform some basic normalisation operations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s:  str\n",
    "        text to operate on\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Normalised string\n",
    "    \n",
    "    \"\"\"\n",
    "    s = s.lower() # lower case\n",
    "    # letter repetition (>2)\n",
    "    s  = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n",
    "    # non word repetition\n",
    "    s = s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n",
    "    # noise text\n",
    "    s = re.sub(r' ing ', ' ', s)\n",
    "    # phrase repetition\n",
    "    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "def func_punc(w_list):\n",
    "    \"\"\"\n",
    "    Remove non-alphabet characters. Includes punctuation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        list of tokens to be processed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        list without non-alphabet characters\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word.isalpha()]\n",
    "\n",
    "def func_stopf(w_list):\n",
    "    \"\"\"\n",
    "    Remove stop words\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        list of tokens to be processed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        list without stop words\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    w_list  = [f for f in w_list if f not in stop_words]\n",
    "    return w_list\n",
    "\n",
    "# stemming\n",
    "pstem = PorterStemmer()\n",
    "\n",
    "def func_stem(w_list):\n",
    "    \"\"\"\n",
    "    stem word list\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        word list for stemming\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        stemmed word list \n",
    "    \"\"\"\n",
    "    sw_list = [pstem.stem(w) for w in w_list]\n",
    "    return sw_list\n",
    "\n",
    "# selecting nouns\n",
    "def func_noun(w_list):\n",
    "    \"\"\"\n",
    "    in: word list to be processed\n",
    "    out: w_list with only nouns selected\n",
    "    \"\"\"\n",
    "    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n",
    "\n",
    "def func_verb(w_list):\n",
    "    \"\"\" \n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        list of tokens to be processed\n",
    "    Returns\n",
    "    -------\n",
    "        list of verbs only\n",
    "    \"\"\"\n",
    "    return [word for (word,pos) in nltk.pos_tag(w_list) if pos[:2] == 'VB']\n",
    "\n",
    "def func_adjective(w_list):\n",
    "    \"\"\" \n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        list of tokens to be processed\n",
    "    Returns\n",
    "    -------\n",
    "        list of verbs only\n",
    "    \"\"\"\n",
    "    return [word for (word,pos) in nltk.pos_tag(w_list) if pos[:2] == 'JJ']\n",
    "\n",
    "def func_inf_words(w_list):\n",
    "    \"\"\" \n",
    "    Retain verbs, adjectives and nouns only\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        list of words to be processed\n",
    "    Returns\n",
    "    -------\n",
    "    list of nouns, adjectives and verbs only\n",
    "    \"\"\"\n",
    "    return [word for (word,pos) in nltk.pos_tag(w_list) if 'VB' in pos or 'JJ' in pos or 'NN' in pos]\n",
    "\n",
    "# spell checker/typo correction\n",
    "def func_spell(w_list):\n",
    "    \"\"\"\n",
    "    in: word list to be processed\n",
    "    out: w_list with typo fixed by symspell. words with no match up will be dropped\n",
    "    \"\"\"\n",
    "    w_list_fixed = []\n",
    "    for word in w_list:\n",
    "        if any(map(word.__contains__, ['covid','lockdown'])):\n",
    "            w_list_fixed.append(word)\n",
    "        else:\n",
    "            suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n",
    "            if suggestions:\n",
    "                w_list_fixed.append(suggestions[0].term)\n",
    "            else:\n",
    "                pass\n",
    "    return w_list_fixed\n",
    "\n",
    "def get_pos_tag(tag):\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def func_lemmatize(word_list):\n",
    "    \"\"\"\n",
    "    Lemmatize word list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word_list:  list\n",
    "        words to process\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Lemmatized word list.\n",
    "\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_list = nltk.pos_tag(word_list)\n",
    "    words_lemmatized = [lemmatizer.lemmatize(word,get_pos_tag(tag))\n",
    "                        for (word,tag) in word_list]\n",
    "    return words_lemmatized\n",
    "\n",
    "def preprocess_sent(rw):\n",
    "    \"\"\"\n",
    "    Get sentence level preprocessed data from raw texts\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        rw: str\n",
    "            sentence to be processed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        sentence level pre-processed text\n",
    "    \"\"\"\n",
    "    s = func_norm(rw)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_word(s):\n",
    "    \"\"\"\n",
    "    Get word level preprocessed data from preprocessed sentences.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s:  str\n",
    "        sentence to be processed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        word level pre-processed text\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    w_list = word_tokenize(s)\n",
    "    w_list = func_punc(w_list)\n",
    "   # w_list = func_inf_words(w_list)\n",
    "    w_list = func_spell(w_list)\n",
    "    w_list = func_lemmatize(w_list)\n",
    "    w_list = func_stopf(w_list)\n",
    "\n",
    "    return w_list\n",
    "\n",
    "def preprocess(docs):\n",
    "    \"\"\"\n",
    "    Preprocess the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs: list\n",
    "        list of documents to be preprocessed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Preprocessed sentences, tokens\n",
    "    \"\"\"\n",
    "    print('Preprocessing raw texts ...')\n",
    "    #n_docs = len(docs)\n",
    "    sentences = []  # sentence level preprocessed\n",
    "    token_lists = []  # word level preprocessed\n",
    "    #samp = np.random.choice(n_docs)\n",
    "    for i in range(0, len(docs)):\n",
    "        sentence = preprocess_sent(docs.iloc[i])\n",
    "        token_list = preprocess_word(sentence)\n",
    "        if token_list:\n",
    "            sentences.append(sentence)\n",
    "            token_lists.append(token_list)\n",
    "        print('{} %'.format(str(np.round((i + 1) / len(writing_df.writing) * 100, 2))), end='\\r')\n",
    "    print('Preprocessing raw texts. Done!')\n",
    "    return sentences, token_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikit_posthocs as sp\n",
    "from scipy import stats\n",
    "from textblob import TextBlob\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "def kruskal_wallis_func(in_df, group_col, test_col):\n",
    "    \"\"\"\n",
    "    Kruskal Wallis test and\n",
    "    post-hoc Dunn's.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd DataFrame\n",
    "        input dataframe\n",
    "    group_col:  str\n",
    "        name of group column\n",
    "    test_col:   str\n",
    "        name of column containing\n",
    "        relevant values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Statistic, pvalue\n",
    "    \"\"\"\n",
    "    data = in_df.pivot(columns = group_col, values = test_col)\n",
    "    if len(in_df[group_col].unique())>2:\n",
    "            statistic,pval = stats.kruskal(data.iloc[:,0],data.iloc[:,1],\n",
    "            data.iloc[:,2],nan_policy = 'omit')\n",
    "            posthoc = sp.posthoc_dunn(\n",
    "                        [data.iloc[:,0].dropna(),data.iloc[:,1].dropna(),data.iloc[:,2].dropna()],\n",
    "                        p_adjust = 'bonferroni'\n",
    "                        )\n",
    "            key = [data.columns[0],data.columns[1],data.columns[2]]\n",
    "    else:\n",
    "        statistic,pval = stats.kruskal(data.iloc[:,0],data.iloc[:,1],\n",
    "        nan_policy = 'omit')\n",
    "        posthoc = None\n",
    "        key = None\n",
    "    return statistic,pval, posthoc, key\n",
    "\n",
    "\n",
    "def get_sentiment(in_df, in_col):\n",
    "    \"\"\"\n",
    "    Get subjectivity\n",
    "    and polarity scores.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd DataFrame\n",
    "        DataFrame to operate on\n",
    "    in_col: str\n",
    "        column holding text data\n",
    "        to operate on\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Input DataFrame with\n",
    "    subjectivity/polarity columns\n",
    "    added.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Getting sentiment scores...\")\n",
    "    in_df = in_df.assign(\n",
    "                        polarity = in_df[in_col].astype('str').apply(\n",
    "                        [lambda x: TextBlob(x).sentiment.polarity]),\n",
    "                        subjectivity = in_df[in_col].astype('str').apply(\n",
    "                        [lambda x: TextBlob(x).sentiment.subjectivity])\n",
    "                        )\n",
    "    print(\"Done!\")\n",
    "    return in_df\n",
    "\n",
    "def run_mixedlm(in_df,group_name,formula, re_intercept):\n",
    "    \"\"\" \n",
    "    Run statsmodels LMEM.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pandas DataFrame\n",
    "        input dataframe\n",
    "    group_name: str\n",
    "        column to group by\n",
    "    formula:    str\n",
    "        patsy formula\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MixedLMResults instance\n",
    "    \"\"\"\n",
    "    if re_intercept:\n",
    "        model = smf.mixedlm(\n",
    "                            formula, in_df, groups = group_name,re_formula = re_intercept, missing = 'drop'\n",
    "                            ).fit()\n",
    "    else:\n",
    "        model = smf.mixedlm(formula, in_df, groups = group_name,missing = 'drop').fit()\n",
    "    return model\n",
    "\n",
    "def run_gee(in_df,group_name,formula,cov_structure, resp_family):\n",
    "    \"\"\" \n",
    "    Run statsmodels GEE.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pandas DataFrame\n",
    "        input dataframe\n",
    "    group_name: str\n",
    "        column to group by\n",
    "    formula:    str\n",
    "        patsy formula\n",
    "    cov_structure:  sm covariance structure\n",
    "        covariance structure (e.g. sm.cov_struct.Independence())\n",
    "    resp_family:    sm family (e.g. sm.families.Tweedie())\n",
    "        mean response structure distribution\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    model = smf.gee(formula,group_name, in_df, cov_struct = cov_structure, family = resp_family,missing = 'drop').fit()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def flatten_list(list_of_lists):\n",
    "    \"\"\"\n",
    "    Flatten list of lists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_of_lists:  list\n",
    "        list of lists to flatten\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Flattened list.\n",
    "    \"\"\"\n",
    "    return [item for sub_list in list_of_lists for item in sub_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn import feature_extraction, model_selection, pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "class Word_Analyzer:\n",
    "    \"\"\"\n",
    "    Class for word frequency based analysis.\n",
    "    \"\"\"\n",
    "    def __init__(self, day_col_name, group_col_name, token_col_name):\n",
    "        self.day_col = day_col_name\n",
    "        self.group_col = group_col_name\n",
    "        self.token_col = token_col_name\n",
    "\n",
    "    def get_pos_tag(self, tag):\n",
    "        \"\"\"\n",
    "        Get wordnet pos tag.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tag:    str\n",
    "        POS tag (from nltk pos_tag output tuple)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        wordnet pos tag\n",
    "        can be passed to nltk lemmatizer.\n",
    "        \"\"\"\n",
    "\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "    \n",
    "    def func_lemmatize(self, word_list):\n",
    "        \"\"\"\n",
    "        Lemmatize word list.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        word_list:  list\n",
    "            words to process\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Lemmatized word list.\n",
    "\n",
    "        \"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        word_list = nltk.pos_tag(word_list)\n",
    "        words_lemmatized = [lemmatizer.lemmatize(word,get_pos_tag(tag))\n",
    "                            for (word,tag) in word_list]\n",
    "        return words_lemmatized\n",
    "    \n",
    "    def get_top_words(self, num_day, group_name, in_df, **pos_tag_type):\n",
    "        \"\"\" \n",
    "        Get an ordered list of words in document.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_day:    int\n",
    "            Day of writing (1, 2, 3 or 4)\n",
    "        group_name: str\n",
    "            The group to process (EW, EWRE or CTR)\n",
    "        in_df:  pd DataFrame\n",
    "            input dataframe containing rel data\n",
    "        **pos_tag_type: list of str\n",
    "            If processing only nouns/verbs/adjectives\n",
    "            pass tag to function using kwargs.\n",
    "            For adjectives, use:\n",
    "            'JJ'\n",
    "            For verbs, use:\n",
    "            'VB'\n",
    "            For nouns, use:\n",
    "            'NN'\n",
    "            If all of the above, pass list:\n",
    "            ['NN','JJ','VB']\n",
    "            as kwarg.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of words, list of vals\n",
    "        words = words ordered from most frequent to rare\n",
    "        vals = corresponding frequency\n",
    "        \"\"\"\n",
    "        token_list = [\n",
    "                    item for sublist in\n",
    "                    [*in_df.loc[\n",
    "                    (in_df[self.group_col] == group_name) &\n",
    "                    (in_df[self.day_col] == num_day),\n",
    "                    self.token_col]]\n",
    "                    for item in sublist\n",
    "                    ]\n",
    "\n",
    "    \n",
    "        if pos_tag_type:\n",
    "            selected_list = []\n",
    "            for tag_type in pos_tag_type.values():\n",
    "                w_list = [\n",
    "                        word for (word,pos) \n",
    "                        in nltk.pos_tag(token_list)\n",
    "                        for tag in tag_type\n",
    "                        if pos[:2] == tag\n",
    "                        ]\n",
    "                selected_list.extend(w_list)\n",
    "        else:\n",
    "            selected_list = token_list\n",
    "        freqs = FreqDist(selected_list)\n",
    "        common_tups = freqs.most_common()\n",
    "        self.common_words = list(zip(*common_tups))[0]\n",
    "        self.common_vals = list(zip(*common_tups))[1]\n",
    "        return self.common_words, self.common_vals\n",
    "    \n",
    "    def func_top_words(self, in_df, pos_tags, visualize):\n",
    "        \"\"\"\n",
    "        Put top 50 words in dataframe,\n",
    "        with option to visualize using barplots.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_df:  pd DataFrame\n",
    "        input dataframe\n",
    "        pos_tags:   list\n",
    "            list of pos tags to use\n",
    "            can be VB, JJ, NN or\n",
    "            any combination (or all) of these\n",
    "        visualize:  int\n",
    "        1 if visualization is needed\n",
    "        0 otherwise\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Datframe of 50 top words\n",
    "        and their frequencies for \n",
    "        all days and conditions.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        top_50_words = []\n",
    "        top_50_vals = []\n",
    "        condition = []\n",
    "        days = []\n",
    "\n",
    "        for group_name in in_df[self.group_col].unique():\n",
    "            for day in in_df[self.day_col].unique():\n",
    "                words,vals= self.get_top_words(day, group_name, in_df, pos_tags = pos_tags)\n",
    "                top_50_words.append(list(words[:50]))\n",
    "                top_50_vals.append(list(vals[:50]))\n",
    "                condition.append(np.repeat(group_name,50))\n",
    "                days.append(np.repeat(day,50))\n",
    "\n",
    "        data = {\n",
    "            'words': flatten_list(top_50_words),\n",
    "            'vals': flatten_list(top_50_vals),\n",
    "            'day': flatten_list(days),\n",
    "            'group': flatten_list(condition)\n",
    "            }\n",
    "        self.most_common_words_df = pd.DataFrame(data)\n",
    "\n",
    "        if visualize == 1:\n",
    "            for num_day in self.most_common_words_df.day.unique():\n",
    "                fig,axes = plt.subplots(3,1,figsize = (30,15),sharey = True)\n",
    "                for i, group_name in enumerate(self.most_common_words_df.group.unique()):\n",
    "                    data = self.most_common_words_df.loc[\n",
    "                                                    (self.most_common_words_df.day==num_day) &\n",
    "                                                    (self.most_common_words_df.group == group_name),\n",
    "                                                    ['words','vals']\n",
    "                                                    ]\n",
    "                    sns.barplot(ax=axes[i], x=data.words, y=data.vals)\n",
    "                    axes[i].set_title(f'Condition: {group_name}, Day: {num_day}')\n",
    "\n",
    "        return self.most_common_words_df\n",
    "    \n",
    "    def print_top10(self, vectorizer, clf, class_labels):\n",
    "        \"\"\"\n",
    "        Prints features with the highest coefficient values,\n",
    "        per class\n",
    "\n",
    "        \"\"\"\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "        for i, class_label in enumerate(class_labels):\n",
    "            top10 = np.argsort(clf.coef_[i])[-15:]\n",
    "            print(\"%s: %s\" % (class_label,\n",
    "                \" \".join(feature_names[j] for j in top10)))\n",
    "\n",
    "    def tf_idf_scores(self, in_df, writing_col):\n",
    "        \"\"\"\n",
    "        Classify statements using Linear SVC\n",
    "        and print top 10 distinguishing features\n",
    "\n",
    "        in_df:  pd DataFrame\n",
    "            input dataframe\n",
    "        \n",
    "        writing_col:    str\n",
    "            name of column containing written statements\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dataframe containing predictions\n",
    "        and actual class labels\n",
    "        for holdout test set\n",
    "        \"\"\"\n",
    "        wrdf_train,wrdf_test = model_selection.train_test_split(in_df.loc[:,[writing_col, 'Group']],\n",
    "                                                                test_size = 0.3,random_state = 35,\n",
    "                                                                stratify = in_df['Group'])\n",
    "        print(wrdf_train.head())\n",
    "        y_train = wrdf_train[self.group_col]\n",
    "        y_test = wrdf_test[self.group_col]\n",
    "        vectorizer_tf_idf = feature_extraction.text.TfidfVectorizer(sublinear_tf = True)\n",
    "        #features = vectorizer_tf_idf.fit_transform(corpus)\n",
    "        #feats_df = pd.DataFrame(features[0].T.todense(), index=vectorizer_tf_idf.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "        #feats_df = feats_df.sort_values('TF-IDF', ascending=False)\n",
    "        clf = LinearSVC(C=1.0, class_weight=\"balanced\")\n",
    "        tf_idf = pipeline.Pipeline([('tfidf', vectorizer_tf_idf),(\"classifier\", clf)])\n",
    "        tf_idf.fit(wrdf_train, y_train)\n",
    "        predicted = tf_idf.predict(wrdf_test)\n",
    "\n",
    "        res_df = pd.DataFrame({'actual': y_test.values, 'predicted': predicted})\n",
    "\n",
    "        self.print_top10(vectorizer_tf_idf,clf,in_df[self.group_col].unique())\n",
    "\n",
    "        return res_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = 0\n",
    "if home:\n",
    "    infiledir = r\"C:\\Users\\Luzia T\\UCL\\WorkingFromHome\\Possible_online_studies\\NLP_expressive_writing\\analysis\\Processed_2\"\n",
    "    writing_dir = r\"C:\\Users\\Luzia T\\UCL\\WorkingFromHome\\Possible_online_studies\\NLP_expressive_writing\\analysis\\writing_data\\statements\"\n",
    "else:\n",
    "    infiledir = r\"P:\\EW_analysis\\analysis\\Processed_2\"\n",
    "    writing_dir = r\"P:\\EW_analysis\\analysis\\writing\\writing_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_df = pd.read_csv(os.path.join(writing_dir, 'writing_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences,tokens = preprocess(writing_df.writing)\n",
    "writing_df = writing_df.assign(\n",
    "                                writing_tokens=tokens,\n",
    "                                writing_sents = sentences\n",
    "                                )\n",
    "writing_df = writing_df.assign(\n",
    "                                word_count = writing_df.writing_tokens.apply(len)\n",
    "                                )\n",
    "writing_df = writing_df.assign(\n",
    "                                word_count_raw =\n",
    "                                writing_df.writing.apply(lambda x: len(x.split()))\n",
    "                                )\n",
    "writing_df = get_sentiment(writing_df,'writing')\n",
    "for val in ['word_count','word_count_raw','polarity','subjectivity']:\n",
    "        # check whether word count is significantly different between conditions:\n",
    "        _, pval, posthoc, key = kruskal_wallis_func(\n",
    "                                        writing_df,'Group', val\n",
    "                                        )\n",
    "        print(f\"\\nP value ({val}) is {pval}.\")\n",
    "        if pval<0.05:\n",
    "                print(f\"Conditions differ significantly on {val}.\")\n",
    "                print(f\"Posthoc ({val}) is:\\n{posthoc}.\")\n",
    "                print(f\"The key is 1 = {key[0]}, 2 = {key[1]}, 3 = {key[2]}\")\n",
    "        else:\n",
    "                print(f\"No significant between group differences on {val}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-depth analysis of between-condition differences (EW/EWRE)\n",
    "(i) There should be no differences between EW & EWRE on D1.\n",
    "(ii) D2: effects/responsibility\n",
    "(iii) D3: different angles/perspectives\n",
    "(iv) D4: learnt/gained/future perspectives\n",
    "Change in most frequent words?\n",
    "Change in polarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day-by-day changes, comparing EW and EWRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Results: GEE\n",
      "===========================================================================\n",
      "Model:                  GEE                  AIC:                -720.9234 \n",
      "Link Function:          identity             BIC:                -1804.5687\n",
      "Dependent Variable:     polarity             Log-Likelihood:     368.46    \n",
      "Date:                   2022-01-03 18:16     LL-Null:            361.83    \n",
      "No. Observations:       321                  Deviance:           1.8924    \n",
      "Df Model:               7                    Pearson chi2:       1.89      \n",
      "Df Residuals:           313                  Scale:              0.0060460 \n",
      "Method:                 IRLS                                               \n",
      "---------------------------------------------------------------------------\n",
      "                              Coef.  Std.Err.    z    P>|z|   [0.025 0.975]\n",
      "---------------------------------------------------------------------------\n",
      "Intercept                     0.0400   0.0102  3.9338 0.0001  0.0201 0.0600\n",
      "C(day)[T.2]                   0.0060   0.0156  0.3843 0.7008 -0.0245 0.0365\n",
      "C(day)[T.3]                  -0.0038   0.0165 -0.2295 0.8184 -0.0361 0.0286\n",
      "C(day)[T.4]                  -0.0058   0.0156 -0.3705 0.7110 -0.0363 0.0247\n",
      "C(Group)[T.EWRE]              0.0006   0.0140  0.0402 0.9679 -0.0269 0.0280\n",
      "C(day)[T.2]:C(Group)[T.EWRE]  0.0087   0.0214  0.4072 0.6839 -0.0333 0.0508\n",
      "C(day)[T.3]:C(Group)[T.EWRE]  0.0307   0.0212  1.4484 0.1475 -0.0108 0.0723\n",
      "C(day)[T.4]:C(Group)[T.EWRE]  0.0471   0.0223  2.1109 0.0348  0.0034 0.0908\n",
      "===========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_polarity_gee = run_gee(\n",
    "                            writing_df[writing_df.Group.isin(['EW','EWRE'])],\n",
    "                            \"id\", \"polarity ~ C(day) * C(Group)\",\n",
    "                            cov_structure = sm.cov_struct.Independence(),\n",
    "                            resp_family = sm.families.Gaussian()\n",
    "                            )\n",
    "\n",
    "print(model_polarity_gee.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa = Word_Analyzer('day','Group','writing_tokens')\n",
    "top_adj_df = wa.func_top_words(writing_df, ['JJ'],0)\n",
    "top_vbs_df = wa.func_top_words(writing_df,['VB'], 0)\n",
    "top_nns_df = wa.func_top_words(writing_df, ['NN'],0)\n",
    "top_words_df = wa.func_top_words(writing_df, ['VB','JJ','NN'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Group'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-141-9b872a4239e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mres_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_idf_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwriting_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'writing'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-94-863e24eaa827>\u001b[0m in \u001b[0;36mtf_idf_scores\u001b[1;34m(self, in_df, writing_col)\u001b[0m\n\u001b[0;32m    210\u001b[0m                                                                 \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m35\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                                                                 stratify = in_df[self.group_col])\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup_col\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m         \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup_col\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[0mvectorizer_tf_idf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msublinear_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 853\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    962\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Group'"
     ]
    }
   ],
   "source": [
    "res_df = wa.tf_idf_scores(writing_df,'writing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrdf_train,wrdf_test = model_selection.train_test_split(writing_df.loc[:,['writing', wa.group_col]],\n",
    "                                                                test_size = 0.3,random_state = 35,\n",
    "                                                                stratify = writing_df[wa.group_col])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ec994d0b4d2f6c713992b16f2c88007ac3578bc75aec34272bb3a023418f6ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
