{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pkg_resources\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "if sym_spell.word_count:\n",
    "    pass\n",
    "else:\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# normalization\n",
    "def func_norm(s):\n",
    "    \"\"\"\n",
    "    Perform some basic normalisation operations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s:  str\n",
    "        text to operate on\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Normalised string\n",
    "    \n",
    "    \"\"\"\n",
    "    s = s.lower() # lower case\n",
    "    # letter repetition (>2)\n",
    "    s  = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n",
    "    # non word repetition\n",
    "    s = s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n",
    "    # noise text\n",
    "    s = re.sub(r' ing ', ' ', s)\n",
    "    # phrase repetition\n",
    "    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "def func_punc(w_list):\n",
    "    \"\"\"\n",
    "    Remove non-alphabet characters. Includes punctuation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        list of tokens to be processed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        list without non-alphabet characters\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word.isalpha()]\n",
    "\n",
    "def func_stopf(w_list):\n",
    "    \"\"\"\n",
    "    Remove stop words\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        list of tokens to be processed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        list without stop words\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    w_list  = [f for f in w_list if f not in stop_words]\n",
    "    return w_list\n",
    "\n",
    "# stemming\n",
    "pstem = PorterStemmer()\n",
    "\n",
    "def func_stem(w_list):\n",
    "    \"\"\"\n",
    "    stem word list\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w_list: list\n",
    "        word list for stemming\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        stemmed word list \n",
    "    \"\"\"\n",
    "    sw_list = [pstem.stem(w) for w in w_list]\n",
    "    return sw_list\n",
    "\n",
    "# selecting nouns\n",
    "def func_noun(w_list):\n",
    "    \"\"\"\n",
    "    in: word list to be processed\n",
    "    out: w_list with only nouns selected\n",
    "    \"\"\"\n",
    "    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n",
    "\n",
    "# spell checker/typo correction\n",
    "def func_spell(w_list):\n",
    "    \"\"\"\n",
    "    in: word list to be processed\n",
    "    out: w_list with typo fixed by symspell. words with no match up will be dropped\n",
    "    \"\"\"\n",
    "    w_list_fixed = []\n",
    "    for word in w_list:\n",
    "        if 'covid' in word:\n",
    "            w_list_fixed.append(word)\n",
    "        else:\n",
    "            suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n",
    "            if suggestions:\n",
    "                w_list_fixed.append(suggestions[0].term)\n",
    "            else:\n",
    "                pass\n",
    "    return w_list_fixed\n",
    "\n",
    "def preprocess_sent(rw):\n",
    "    \"\"\"\n",
    "    Get sentence level preprocessed data from raw texts\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        rw: str\n",
    "            sentence to be processed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        sentence level pre-processed text\n",
    "    \"\"\"\n",
    "    s = func_norm(rw)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_word(s):\n",
    "    \"\"\"\n",
    "    Get word level preprocessed data from preprocessed sentences.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s:  str\n",
    "        sentence to be processed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        word level pre-processed text\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    w_list = word_tokenize(s)\n",
    "    w_list = func_punc(w_list)\n",
    "    w_list = func_noun(w_list)\n",
    "    w_list = func_spell(w_list)\n",
    "    w_list = func_stem(w_list)\n",
    "    w_list = func_stopf(w_list)\n",
    "\n",
    "    return w_list\n",
    "\n",
    "def preprocess(docs):\n",
    "    \"\"\"\n",
    "    Preprocess the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs: list\n",
    "        list of documents to be preprocessed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Preprocessed sentences, tokens\n",
    "    \"\"\"\n",
    "    print('Preprocessing raw texts ...')\n",
    "    #n_docs = len(docs)\n",
    "    sentences = []  # sentence level preprocessed\n",
    "    token_lists = []  # word level preprocessed\n",
    "    #samp = np.random.choice(n_docs)\n",
    "    for i in range(0, len(docs)):\n",
    "        sentence = preprocess_sent(docs.iloc[i])\n",
    "        token_list = preprocess_word(sentence)\n",
    "        if token_list:\n",
    "            sentences.append(sentence)\n",
    "            token_lists.append(token_list)\n",
    "        print('{} %'.format(str(np.round((i + 1) / len(writing_df.writing) * 100, 2))), end='\\r')\n",
    "    print('Preprocessing raw texts. Done!')\n",
    "    return sentences, token_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikit_posthocs as sp\n",
    "from scipy import stats\n",
    "from textblob import TextBlob\n",
    "\n",
    "def kruskal_wallis_func(in_df, group_col, test_col):\n",
    "    \"\"\"\n",
    "    Kruskal Wallis test and\n",
    "    post-hoc Dunn's.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd DataFrame\n",
    "        input dataframe\n",
    "    group_col:  str\n",
    "        name of group column\n",
    "    test_col:   str\n",
    "        name of column containing\n",
    "        relevant values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Statistic, pvalue\n",
    "    \"\"\"\n",
    "    data = in_df.pivot(columns = group_col, values = test_col)\n",
    "    if len(in_df[group_col].unique())>2:\n",
    "            statistic,pval = stats.kruskal(data.iloc[:,0],data.iloc[:,1],\n",
    "            data.iloc[:,2],nan_policy = 'omit')\n",
    "            posthoc = sp.posthoc_dunn(\n",
    "                        [data.iloc[:,0].dropna(),data.iloc[:,1].dropna(),data.iloc[:,2].dropna()],\n",
    "                        p_adjust = 'bonferroni'\n",
    "                        )\n",
    "            key = [data.columns[0],data.columns[1],data.columns[2]]\n",
    "    else:\n",
    "        statistic,pval = stats.kruskal(data.iloc[:,0],data.iloc[:,1],\n",
    "        nan_policy = 'omit')\n",
    "        posthoc = None\n",
    "        key = None\n",
    "    return statistic,pval, posthoc, key\n",
    "\n",
    "\n",
    "def get_sentiment(in_df, in_col):\n",
    "    \"\"\"\n",
    "    Get subjectivity\n",
    "    and polarity scores.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd DataFrame\n",
    "        DataFrame to operate on\n",
    "    in_col: str\n",
    "        column holding text data\n",
    "        to operate on\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Input DataFrame with\n",
    "    subjectivity/polarity columns\n",
    "    added.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Getting sentiment scores...\")\n",
    "    in_df = in_df.assign(\n",
    "                        polarity = in_df[in_col].astype('str').apply(\n",
    "                        [lambda x: TextBlob(x).sentiment.polarity]),\n",
    "                        subjectivity = in_df[in_col].astype('str').apply(\n",
    "                        [lambda x: TextBlob(x).sentiment.subjectivity])\n",
    "                        )\n",
    "    print(\"Done!\")\n",
    "    return in_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = 1\n",
    "if home:\n",
    "    infiledir = r\"C:\\Users\\Luzia T\\UCL\\WorkingFromHome\\Possible_online_studies\\NLP_expressive_writing\\analysis\\Processed_2\"\n",
    "    writing_dir = r\"C:\\Users\\Luzia T\\UCL\\WorkingFromHome\\Possible_online_studies\\NLP_expressive_writing\\analysis\\writing_data\\statements\"\n",
    "else:\n",
    "    infiledir = r\"P:\\EW_analysis\\analysis\\Processed_2\"\n",
    "    writing_dir = r\"P:\\EW_analysis\\analysis\\writing\\writing_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_df = pd.read_csv(os.path.join(writing_dir, 'writing_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P value (word_count) is 0.08764873320117955.\n",
      "No significant between group differences on word_count.\n",
      "\n",
      "P value (word_count_raw) is 0.0029349359121382448.\n",
      "Conditions differ significantly on word_count_raw.\n",
      "Posthoc (word_count_raw) is:\n",
      "          1         2         3\n",
      "1  1.000000  0.083669  0.002315\n",
      "2  0.083669  1.000000  0.748153\n",
      "3  0.002315  0.748153  1.000000.\n",
      "The key is 1 = CTR, 2 = EW, 3 = EWRE\n",
      "\n",
      "P value (polarity) is 0.03688590519079081.\n",
      "Conditions differ significantly on polarity.\n",
      "Posthoc (polarity) is:\n",
      "          1        2         3\n",
      "1  1.000000  1.00000  0.124654\n",
      "2  1.000000  1.00000  0.053490\n",
      "3  0.124654  0.05349  1.000000.\n",
      "The key is 1 = CTR, 2 = EW, 3 = EWRE\n",
      "\n",
      "P value (subjectivity) is 1.3978564535849075e-31.\n",
      "Conditions differ significantly on subjectivity.\n",
      "Posthoc (subjectivity) is:\n",
      "              1             2             3\n",
      "1  1.000000e+00  1.510864e-25  1.931276e-23\n",
      "2  1.510864e-25  1.000000e+00  1.000000e+00\n",
      "3  1.931276e-23  1.000000e+00  1.000000e+00.\n",
      "The key is 1 = CTR, 2 = EW, 3 = EWRE\n"
     ]
    }
   ],
   "source": [
    "sentences,tokens = preprocess(writing_df.writing)\n",
    "writing_df = writing_df.assign(\n",
    "                                writing_tokens=tokens,\n",
    "                                writing_sents = sentences\n",
    "                                )\n",
    "writing_df = writing_df.assign(\n",
    "                                word_count = writing_df.writing_tokens.apply(len)\n",
    "                                )\n",
    "writing_df = writing_df.assign(\n",
    "                                word_count_raw =\n",
    "                                writing_df.writing.apply(lambda x: len(x.split()))\n",
    "                                )\n",
    "writing_df = get_sentiment(writing_df,'writing')\n",
    "for val in ['word_count','word_count_raw','polarity','subjectivity']:\n",
    "        # check whether word count is significantly different between conditions:\n",
    "        _, pval, posthoc, key = kruskal_wallis_func(\n",
    "                                        writing_df,'Group', val\n",
    "                                        )\n",
    "        print(f\"\\nP value ({val}) is {pval}.\")\n",
    "        if pval<0.05:\n",
    "                print(f\"Conditions differ significantly on {val}.\")\n",
    "                print(f\"Posthoc ({val}) is:\\n{posthoc}.\")\n",
    "                print(f\"The key is 1 = {key[0]}, 2 = {key[1]}, 3 = {key[2]}\")\n",
    "        else:\n",
    "                print(f\"No significant between group differences on {val}.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ec994d0b4d2f6c713992b16f2c88007ac3578bc75aec34272bb3a023418f6ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
