{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing with the written statements from the Expressive Writing study. \n",
    "\n",
    "sources:\n",
    "https://www.kaggle.com/dskswu/topic-modeling-bert-lda - topic modeling bert+lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = 1\n",
    "if home:\n",
    "    infiledir = r\"C:\\Users\\Luzia T\\UCL\\WorkingFromHome\\Possible_online_studies\\NLP_expressive_writing\\analysis\\Processed_2\"\n",
    "    rand_dir = r\"C:\\Users\\Luzia T\\UCL\\WorkingFromHome\\Possible_online_studies\\NLP_expressive_writing\\analysis\"\n",
    "else:\n",
    "    infiledir = r\"P:\\EW_analysis\\analysis\\Processed_2\"\n",
    "    writing_dir = r\"P:\\EW_analysis\\analysis\\writing\\writing_data\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'writing_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LUZIAT~1\\AppData\\Local\\Temp/ipykernel_9284/2415824447.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdirname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwriting_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'writing_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk(writing_dir):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "ew_files = [f for f in filenames if 'T1' in f]\n",
    "ewre_files = [f for f in filenames if 'T2' in f]\n",
    "ctr_files = [f for f in filenames if 'T3' in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diaries(directory,filenames,group_name):\n",
    "    all_files = []\n",
    "    for i,name in enumerate(filenames):\n",
    "        diary_df = pd.read_csv(os.path.join(directory,name),skiprows = [0,2])\n",
    "        # retain only records of people who finished the survey\n",
    "        diary_df = diary_df.loc[diary_df.Progress==100,:]\n",
    "        # what day of writing were they on\n",
    "        diary_df['day'] = i+1\n",
    "        diary_df['Group'] = group_name\n",
    "        writing_col = diary_df.filter(like = 'Please use the box below').columns[0]\n",
    "        diary_df = diary_df.rename(columns = {writing_col: 'writing'})\n",
    "        id_col = diary_df.filter(like = 'participant ID').columns[0]\n",
    "        diary_df = diary_df.rename(columns = {id_col: 'id'})\n",
    "        all_files.append(diary_df)\n",
    "    return all_files\n",
    "\n",
    "def read_files(infiles):\n",
    "    all_df = pd.concat([pd.DataFrame(infiles[0]), pd.DataFrame(infiles[1]),pd.DataFrame(infiles[2]),pd.DataFrame(infiles[3])])\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_ew = get_diaries(writing_dir, ew_files,'EW')\n",
    "ew_df = read_files(all_ew)\n",
    "all_ewre = get_diaries(writing_dir, ewre_files,'EWRE')\n",
    "ewre_df = read_files(all_ewre)\n",
    "all_ctr = get_diaries(writing_dir, ctr_files,'CTR')\n",
    "ctr_df = read_files(all_ctr)\n",
    "writing_df = pd.concat([ew_df,ewre_df,ctr_df])\n",
    "writing_df.loc[:,'writing'] = writing_df.writing\n",
    "\n",
    "# save if needed\n",
    "save = 0\n",
    "if save:\n",
    "    writing_df.to_csv(os.path.join(writing_dir, 'writing_df.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the file holding the complete data set and the data from each day of writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dat_df = pd.read_csv(os.path.join(infiledir, 'all_dat_df.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pkg_resources\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "if sym_spell.word_count:\n",
    "    pass\n",
    "else:\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# normalization\n",
    "def func_norm(s):\n",
    "    \"\"\"\n",
    "    Some basic normalisation.\n",
    "    Params: input string\n",
    "    Out: normalised string.\n",
    "    \n",
    "    \"\"\"\n",
    "    s = s.lower() # lower case\n",
    "    # letter repetition (>2)\n",
    "    s  = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n",
    "    # non word repetition\n",
    "    s = s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n",
    "    # noise text\n",
    "    s = re.sub(r' ing ', ' ', s)\n",
    "    # phrase repetition\n",
    "    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "def func_punc(w_list):\n",
    "    \"\"\"\n",
    "    w_list: word list to be processed\n",
    "    returns w_list with punctuation and number filter out\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word.isalpha()]\n",
    "\n",
    "def func_stopf(w_list):\n",
    "    \"\"\"\n",
    "    filter stop words\n",
    "    in:  word list\n",
    "    out: word list w/o stop words\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    w_list  = [f for f in w_list if f not in stop_words]\n",
    "    return w_list\n",
    "\n",
    "# stemming\n",
    "pstem = PorterStemmer()\n",
    "\n",
    "def func_stem(w_list):\n",
    "    \"\"\"\n",
    "    stem word list\n",
    "    in: word list for stemming\n",
    "    out: stemmed word list \n",
    "    \"\"\"\n",
    "    sw_list = [pstem.stem(w) for w in w_list]\n",
    "    return sw_list\n",
    "\n",
    "# selecting nouns\n",
    "def func_noun(w_list):\n",
    "    \"\"\"\n",
    "    w_list: word list to be processed\n",
    "    return: w_list with only nouns selected\n",
    "    \"\"\"\n",
    "    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n",
    "\n",
    "# spell checker/typo correction\n",
    "def func_spell(w_list):\n",
    "    \"\"\"\n",
    "    w_list: word list to be processed\n",
    "    return: w_list with typo fixed by symspell. words with no match up will be dropped\n",
    "    \"\"\"\n",
    "    w_list_fixed = []\n",
    "    for word in w_list:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n",
    "        if suggestions:\n",
    "            w_list_fixed.append(suggestions[0].term)\n",
    "        else:\n",
    "            pass\n",
    "    return w_list_fixed\n",
    "\n",
    "def preprocess_sent(rw):\n",
    "    \"\"\"\n",
    "    Get sentence level preprocessed data from raw texts\n",
    "    param rw: text to be procssed\n",
    "    return: sentence level pre-processed text\n",
    "    \"\"\"\n",
    "    s = func_norm(rw)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_word(s):\n",
    "    \"\"\"\n",
    "    Get word level preprocessed data from preprocessed sentences\n",
    "    including: remove punctuation, select noun, fix typo, stem, stop_words\n",
    "    :param s: sentence to be processed\n",
    "    :return: word level pre-processed review\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    w_list = word_tokenize(s)\n",
    "    w_list = func_punc(w_list)\n",
    "    #w_list = func_noun(w_list)\n",
    "    w_list = func_spell(w_list)\n",
    "    w_list = func_stem(w_list)\n",
    "    w_list = func_stopf(w_list)\n",
    "\n",
    "    return w_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "\n",
    "def preprocess(docs):\n",
    "    \"\"\"\n",
    "    Preprocess the data\n",
    "    \"\"\"\n",
    "    print('Preprocessing raw texts ...')\n",
    "    #n_docs = len(docs)\n",
    "    sentences = []  # sentence level preprocessed\n",
    "    token_lists = []  # word level preprocessed\n",
    "    idx_in = []  # index of sample selected\n",
    "    #samp = np.random.choice(n_docs)\n",
    "    for i in range(0, len(docs)):\n",
    "        sentence = preprocess_sent(docs.iloc[i])\n",
    "        token_list = preprocess_word(sentence)\n",
    "        if token_list:\n",
    "            idx_in.append(i)\n",
    "            sentences.append(sentence)\n",
    "            token_lists.append(token_list)\n",
    "        print('{} %'.format(str(np.round((i + 1) / len(writing_df.writing) * 100, 2))), end='\\r')\n",
    "    print('Preprocessing raw texts. Done!')\n",
    "    return sentences, token_lists, idx_in\n",
    "\n",
    "# define model object\n",
    "class Topic_Model:\n",
    "    def __init__(self, k=10, method='TFIDF'):\n",
    "        \"\"\"\n",
    "        :param k: number of topics\n",
    "        :param method: method chosen for the topic model\n",
    "        \"\"\"\n",
    "        if method not in {'TFIDF', 'LDA', 'BERT', 'LDA_BERT'}:\n",
    "            raise Exception('Invalid method!')\n",
    "        self.k = k\n",
    "        self.dictionary = None\n",
    "        self.corpus = None\n",
    "        self.cluster_model = None\n",
    "        self.tfidfmodel = None\n",
    "        self.ldamodel = None\n",
    "        self.vec = {}\n",
    "       # self.gamma = 15  # parameter for relative importance of lda\n",
    "        self.method = method\n",
    "        self.AE = None\n",
    "        self.id = method + '_' + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    def get_vector(self,sentences,token_lists,method = None):\n",
    "        \"\"\"\n",
    "        In: list of tokenized text\n",
    "        returns: dictionary & corpus\n",
    "        \"\"\" \n",
    "        self.dictionary = Dictionary(token_lists)\n",
    "        print('done')\n",
    "        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
    "        if not method:\n",
    "            method = 'TFIDF'\n",
    "            print(\"Using default method (TFIDF)\", method)\n",
    "        elif method not in ['TFIDF','LDA']:\n",
    "            raise Exception('Invalid method!')\n",
    "        if method == 'TFIDF':\n",
    "            model = models.TfidfModel(self.corpus)  # fit model\n",
    "            vec = [model[text] for text in self.corpus]\n",
    "            return vec\n",
    "        elif method == 'LDA':\n",
    "            print('Getting vector representations for LDA ...')\n",
    "            if not self.ldamodel:\n",
    "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
    "                                                                passes=20)\n",
    "\n",
    "            def get_vec_lda(model, corpus, k):\n",
    "                \"\"\"\n",
    "                get the LDA vector representation (probabilistic topic assignments for all documents)\n",
    "                :return: vec_lda with dimension: (n_doc * n_topic)\n",
    "                \"\"\"\n",
    "                n_doc = len(corpus)\n",
    "                vec_lda = np.zeros((n_doc, k))\n",
    "                for i in range(n_doc):\n",
    "                 # get the distribution for the i-th document in corpus\n",
    "                    for topic, prob in model.get_document_topics(corpus[i]):\n",
    "                        vec_lda[i, topic] = prob\n",
    "                return vec_lda\n",
    "\n",
    "            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n",
    "            print('Getting vector representations for LDA. Done!')\n",
    "            return vec\n",
    "\n",
    "        elif method == 'BERT':\n",
    "\n",
    "            print('Getting vector representations for BERT ...')\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            model = SentenceTransformer('bert-base-nli-max-tokens')\n",
    "            vec = np.array(model.encode(sentences, show_progress_bar=True))\n",
    "            print('Getting vector representations for BERT. Done!')\n",
    "            return vec\n",
    "\n",
    "    \n",
    "    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n",
    "        print(method)    \n",
    "        if method is None:\n",
    "            method = self.method\n",
    "        # Default clustering method\n",
    "        if m_clustering is None:\n",
    "            m_clustering = KMeans\n",
    "        \n",
    "        if not self.dictionary:\n",
    "            self.dictionary = Dictionary(token_lists)\n",
    "            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
    "\n",
    "        if method == 'LDA':\n",
    "            if not self.ldamodel:\n",
    "                print('Fitting LDA ...')\n",
    "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
    "                                                                passes=20)\n",
    "                print('Fitting LDA Done!')\n",
    "        if method == 'TFIDF':\n",
    "            self.vec = self.get_vector(sentences,token_lists,method)\n",
    "            print(self.vec)\n",
    "            \n",
    "        else:\n",
    "            print('Clustering embeddings ...')\n",
    "            self.cluster_model = m_clustering(self.k)\n",
    "            self.vec[method] = self.get_vector(sentences, token_lists, method)\n",
    "            self.cluster_model.fit(self.vec[method])\n",
    "            print('Clustering embeddings. Done!')\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ntopic = 10\n",
    "    method = 'TFIDF'\n",
    "    #parser = argparse.ArgumentParser(description='contextual_topic_identification tm_test:1.0')\n",
    "\n",
    "    #parser.add_argument('--fpath', default='/kaggle/working/train.csv')\n",
    "    #parser.add_argument('--ntopic', default=10,)\n",
    "    #parser.add_argument('--method', default='TFIDF')\n",
    "    #parser.add_argument('--samp_size', default=20500)\n",
    "    \n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    #data = writing_df.writing \n",
    "\n",
    "    #sentences, token_lists, idx_in = preprocess(data)\n",
    "\n",
    "    tm = Topic_Model(k = ntopic, method = method)\n",
    "    tm.fit(sentences,token_lists)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48186e61764c8c514947f0ef500accf59797b98e64cdc910e21ec2975c1f1025"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
