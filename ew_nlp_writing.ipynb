{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "- read in data\n",
    "- preprocess \n",
    "- model\n",
    "blabla\n",
    "sources:\n",
    "https://www.kaggle.com/dskswu/topic-modeling-bert-lda - topic modeling bert+lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = 0\n",
    "if home:\n",
    "    infiledir = r\"C:\\Users\\Luzia T\\UCL\\WorkingFromHome\\Possible_online_studies\\NLP_expressive_writing\\analysis\\Processed_2\"\n",
    "    rand_dir = r\"C:\\Users\\Luzia T\\UCL\\WorkingFromHome\\Possible_online_studies\\NLP_expressive_writing\\analysis\"\n",
    "else:\n",
    "    infiledir = r\"P:\\EW_analysis\\analysis\\Processed_2\"\n",
    "    writing_dir = r\"P:\\EW_analysis\\analysis\\writing\\writing_data\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:\\EW_analysis\\analysis\\writing\\writing_data\\Writing_T1_Day1.csv\n",
      "P:\\EW_analysis\\analysis\\writing\\writing_data\\Writing_T1_Day2.csv\n",
      "P:\\EW_analysis\\analysis\\writing\\writing_data\\Writing_T1_Day3.csv\n",
      "P:\\EW_analysis\\analysis\\writing\\writing_data\\Writing_T1_Day4.csv\n",
      "P:\\EW_analysis\\analysis\\writing\\writing_data\\Writing_T2_Day1.csv\n",
      "P:\\EW_analysis\\analysis\\writing\\writing_data\\Writing_T2_Day2.csv\n",
      "P:\\EW_analysis\\analysis\\writing\\writing_data\\Writing_T2_Day3.csv\n",
      "P:\\EW_analysis\\analysis\\writing\\writing_data\\Writing_T2_Day4.csv\n",
      "P:\\EW_analysis\\analysis\\writing\\writing_data\\Writing_T3_Day1.csv\n",
      "P:\\EW_analysis\\analysis\\writing\\writing_data\\Writing_T3_Day2.csv\n",
      "P:\\EW_analysis\\analysis\\writing\\writing_data\\Writing_T3_Day3.csv\n",
      "P:\\EW_analysis\\analysis\\writing\\writing_data\\Writing_T3_Day4.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk(writing_dir):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "ew_files = [f for f in filenames if 'T1' in f]\n",
    "ewre_files = [f for f in filenames if 'T2' in f]\n",
    "ctr_files = [f for f in filenames if 'T3' in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diaries(directory,filenames,group_name):\n",
    "    all_files = []\n",
    "    for i,name in enumerate(filenames):\n",
    "        diary_df = pd.read_csv(os.path.join(directory,name),skiprows = [0,2])\n",
    "        # retain only records of people who finished the survey\n",
    "        diary_df = diary_df.loc[diary_df.Progress==100,:]\n",
    "        # what day of writing were they on\n",
    "        diary_df['day'] = i+1\n",
    "        diary_df['Group'] = group_name\n",
    "        writing_col = diary_df.filter(like = 'Please use the box below').columns[0]\n",
    "        diary_df = diary_df.rename(columns = {writing_col: 'writing'})\n",
    "        all_files.append(diary_df)\n",
    "    return all_files\n",
    "\n",
    "def read_files(infiles):\n",
    "    all_df = pd.concat([pd.DataFrame(infiles[0]), pd.DataFrame(infiles[1]),pd.DataFrame(infiles[2]),pd.DataFrame(infiles[3])])\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_ew = get_diaries(writing_dir, ew_files,'EW')\n",
    "ew_df = read_files(all_ew)\n",
    "all_ewre = get_diaries(writing_dir, ewre_files,'EWRE')\n",
    "ewre_df = read_files(all_ewre)\n",
    "all_ctr = get_diaries(writing_dir, ctr_files,'CTR')\n",
    "ctr_df = read_files(all_ctr)\n",
    "writing_df = pd.concat([ew_df,ewre_df,ctr_df])\n",
    "\n",
    "# save if needed\n",
    "save = 0\n",
    "if save:\n",
    "    writing_df.to_csv(os.path.join(writing_dir, 'writing_df.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the file holding the complete data set and the data from each day of writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dat_df = pd.read_csv(os.path.join(infiledir, 'all_dat_df.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pkg_resources\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "if sym_spell.word_count:\n",
    "    pass\n",
    "else:\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# normalization\n",
    "def func_norm(s):\n",
    "    \"\"\"\n",
    "    Some basic normalisation.\n",
    "    Params: input string\n",
    "    Out: normalised string.\n",
    "    \n",
    "    \"\"\"\n",
    "    s = s.lower() # lower case\n",
    "    # letter repetition (>2)\n",
    "    s  = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n",
    "    # non word repetition\n",
    "    s = s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n",
    "    # noise text\n",
    "    s = re.sub(r' ing ', ' ', s)\n",
    "    # phrase repetition\n",
    "    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "def func_punc(w_list):\n",
    "    \"\"\"\n",
    "    w_list: word list to be processed\n",
    "    returns w_list with punctuation and number filter out\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word.isalpha()]\n",
    "\n",
    "def func_stopf(w_list):\n",
    "    \"\"\"\n",
    "    filter stop words\n",
    "    in:  word list\n",
    "    out: word list w/o stop words\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    w_list  = [f for f in w_list if f not in stop_words]\n",
    "\n",
    "# stemming\n",
    "pstem = PorterStemmer()\n",
    "\n",
    "def func_stem(w_list):\n",
    "    \"\"\"\n",
    "    stem word list\n",
    "    in: word list for stemming\n",
    "    out: stemmed word list \n",
    "    \"\"\"\n",
    "    sw_list = [pstem.stem(w) for w in w_list]\n",
    "    return sw_list\n",
    "\n",
    "# selecting nouns\n",
    "def func_noun(w_list):\n",
    "    \"\"\"\n",
    "    w_list: word list to be processed\n",
    "    return: w_list with only nouns selected\n",
    "    \"\"\"\n",
    "    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n",
    "\n",
    "# spell checker/typo correction\n",
    "def func_spell(w_list):\n",
    "    \"\"\"\n",
    "    w_list: word list to be processed\n",
    "    return: w_list with typo fixed by symspell. words with no match up will be dropped\n",
    "    \"\"\"\n",
    "    w_list_fixed = []\n",
    "    for word in w_list:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n",
    "        if suggestions:\n",
    "            w_list_fixed.append(suggestions[0].term)\n",
    "        else:\n",
    "            pass\n",
    "    return w_list_fixed\n",
    "\n",
    "def preprocess_sent(rw):\n",
    "    \"\"\"\n",
    "    Get sentence level preprocessed data from raw texts\n",
    "    param rw: text to be procssed\n",
    "    return: sentence level pre-processed text\n",
    "    \"\"\"\n",
    "    s = func_norm(rw)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_word(s):\n",
    "    \"\"\"\n",
    "    Get word level preprocessed data from preprocessed sentences\n",
    "    including: remove punctuation, select noun, fix typo, stem, stop_words\n",
    "    :param s: sentence to be processed\n",
    "    :return: word level pre-processed review\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    w_list = word_tokenize(s)\n",
    "    w_list = func_punc(w_list)\n",
    "    w_list = func_noun(w_list)\n",
    "    w_list = func_spell(w_list)\n",
    "    w_list = func_stem(w_list)\n",
    "    w_list = func_stopf(w_list)\n",
    "\n",
    "    return w_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(docs):\n",
    "    \"\"\"\n",
    "    Preprocess the data\n",
    "    \"\"\"\n",
    "\n",
    "    print('Preprocessing raw texts ...')\n",
    "    n_docs = len(docs)\n",
    "    sentences = []  # sentence level preprocessed\n",
    "    token_lists = []  # word level preprocessed\n",
    "    idx_in = []  # index of sample selected\n",
    "    samp = np.random.choice(n_docs, samp_size)\n",
    "    for i, idx in enumerate(samp):\n",
    "        sentence = preprocess_sent(docs[idx])\n",
    "        token_list = preprocess_word(sentence)\n",
    "        if token_list:\n",
    "            idx_in.append(idx)\n",
    "            sentences.append(sentence)\n",
    "            token_lists.append(token_list)\n",
    "        print('{} %'.format(str(np.round((i + 1) / len(samp) * 100, 2))), end='\\r')\n",
    "    print('Preprocessing raw texts. Done!')\n",
    "    return sentences, token_lists, idx_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent,tok, idx_in = preprocess(writing_df.writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    \n",
    "    method = \"LDA_BERT\"\n",
    "    samp_size = 51000\n",
    "    ntopic = 10\n",
    "    \n",
    "    #parser = argparse.ArgumentParser(description='contextual_topic_identification tm_test:1.0')\n",
    "\n",
    "    #parser.add_argument('--fpath', default='/kaggle/working/train.csv')\n",
    "    #parser.add_argument('--ntopic', default=10,)\n",
    "    #parser.add_argument('--method', default='TFIDF')\n",
    "    #parser.add_argument('--samp_size', default=20500)\n",
    "    \n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    data = writing_df.writing #pd.read_csv('/kaggle/working/train.csv')\n",
    "    #data = data.fillna('')  # only the comments has NaN's\n",
    "    rws = data.abstract\n",
    "    sentences, token_lists, idx_in = preprocess(rws, samp_size=samp_size)\n",
    "    # Define the topic model object\n",
    "    #tm = Topic_Model(k = 10), method = TFIDF)\n",
    "    tm = Topic_Model(k = ntopic, method = method)\n",
    "    # Fit the topic model by chosen method\n",
    "    tm.fit(sentences, token_lists)\n",
    "    # Evaluate using metrics\n",
    "    with open(\"/kaggle/working/{}.file\".format(tm.id), \"wb\") as f:\n",
    "        pickle.dump(tm, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('Coherence:', get_coherence(tm, token_lists, 'c_v'))\n",
    "    print('Silhouette Score:', get_silhouette(tm))\n",
    "    # visualize and save img\n",
    "    visualize(tm)\n",
    "    for i in range(tm.k):\n",
    "        get_wordcloud(tm, token_lists, i)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48186e61764c8c514947f0ef500accf59797b98e64cdc910e21ec2975c1f1025"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
