{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.factorplots import interaction_plot\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = 1\n",
    "if home:\n",
    "    infiledir = r\"C:\\Users\\Luzia T\\UCL\\WorkingFromHome\\Possible_online_studies\\NLP_expressive_writing\\analysis\\Processed_2\"\n",
    "    rand_dir = r\"C:\\Users\\Luzia T\\UCL\\WorkingFromHome\\Possible_online_studies\\NLP_expressive_writing\\analysis\"\n",
    "    writing_dir = r\"C:\\Users\\Luzia T\\UCL\\WorkingFromHome\\Possible_online_studies\\NLP_expressive_writing\\analysis\\writing_data\\statements\"\n",
    "    \n",
    "else:\n",
    "    infiledir = r\"P:\\EW_analysis\\analysis\\Processed_2\"\n",
    "    rand_dir = r\"P:\\EW_analysis\\analysis\"\n",
    "    writing_dir = r\"P:\\EW_analysis\\analysis\\writing\\writing_data\"\n",
    "\n",
    "output_dir = os.path.join(os.getcwd(),\"output_dir_mcq\")\n",
    "try:\n",
    "    os.makedirs(output_dir)\n",
    "except OSError:\n",
    "    # if directory already exists\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def score_quest(in_df, out_col_name: str, items = None):\n",
    "    \"\"\"\n",
    "    Calculate summary scores for\n",
    "    questionnaires (MCQ)\n",
    "\n",
    "    in_df:  pd.DataFrame\n",
    "        input dataframe containing scores\n",
    "    out_col_name:   str\n",
    "        Name of output column containing summary score\n",
    "    items:\n",
    "        if supplied, list of items to consider\n",
    "        in calculation of summary scores\n",
    "    \"\"\"\n",
    "    if not items:\n",
    "        # assume want total score\n",
    "        in_df[out_col_name] = in_df.sum(axis =1 )\n",
    "    else:\n",
    "        in_df[out_col_name] = in_df.loc[:, items].sum(axis =1 )\n",
    "\n",
    "def get_polarity(in_df, writing_col, polarity_type: str):\n",
    "    \"\"\"\n",
    "    Get polarity for written statements.\n",
    "    Using NLTK VADER.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd DataFrame\n",
    "        input dataframe containing\n",
    "        written statements\n",
    "    writing_col:    str\n",
    "        name of column containing\n",
    "        statments\n",
    "    polarity_type:  str\n",
    "        polarity to extract\n",
    "        all = get entire polarity_scores\n",
    "        output as a dict\n",
    "        pos = positive only\n",
    "        neg = negative only\n",
    "        neu = neutral only\n",
    "        compound = compound\n",
    "    \"\"\"\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    new_col_name = '_'.join(['sentiment',polarity_type, 'vader'])\n",
    "    if polarity_type == 'all':   \n",
    "        in_df[new_col_name] = in_df[writing_col].apply(lambda x: sid.polarity_scores(x))\n",
    "    else:\n",
    "        in_df[new_col_name] = in_df[writing_col].apply(lambda x: sid.polarity_scores(x)[polarity_type])\n",
    "    \n",
    "    return in_df\n",
    "\n",
    "def scale_features(in_df, col_names, scaler):\n",
    "    \"\"\"\n",
    "    Scale features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pd DataFrame\n",
    "        input datatframe to operate on\n",
    "    col_names: list[str]\n",
    "        list of features to scale\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    in_df with scaled cols added.\n",
    "    \"\"\"\n",
    "    for col in col_names:\n",
    "        in_df[col+'_scaled'] = scaler.fit_transform(in_df[col].values.reshape(-1,1))\n",
    "    return in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Explorer:\n",
    "    def __init__(self, in_df,group_col,time_col):\n",
    "        self.in_df = in_df\n",
    "        self.group_col = group_col\n",
    "        self.time_col = time_col\n",
    "    \n",
    "    def nan_zero_ids(self,response_name:str,zero_or_nan = 'nan'):\n",
    "        \"\"\"\n",
    "        Get ids of participant with\n",
    "        missing scores.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        response_name: str\n",
    "            feature to check nans for.\n",
    "        zero_or_nan: str\n",
    "            check for zero or nan values\n",
    "        \"\"\"\n",
    "        if zero_or_nan == 'nan':\n",
    "            select_ids = self.in_df.loc[self.in_df[response_name].isna(),'id'].values\n",
    "        else:\n",
    "            select_ids = self.in_df.loc[self.in_df[response_name]==0,'id'].values\n",
    "        print(f\"\\nThe percentage of ids with {zero_or_nan} values on feature {response_name} is {(len(set(select_ids))/self.in_df.shape[0])*100}%.\")\n",
    "        if len(select_ids)>0:\n",
    "            select_groups = self.in_df.loc[self.in_df.id.isin(select_ids),self.group_col].value_counts()\n",
    "            print(f\"\\n{zero_or_nan} counts for Groups:\\n{select_groups}\")\n",
    "        return select_ids\n",
    "       \n",
    "    def visualize_dist(self, var_name, distribution = None):\n",
    "        \"\"\"\n",
    "        Draw QQ plot of var_name\n",
    "        against normal (default) or gamma distribution\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        var_name:   str\n",
    "            name of feature to plot\n",
    "        distribution:\n",
    "            if supplied, will plot\n",
    "            var_name against gamma distribution\n",
    "\n",
    "        \"\"\"\n",
    "        response_vals = self.in_df.loc[:,var_name]\n",
    "        if not distribution:\n",
    "            distribution = 'normal'\n",
    "            fig = sm.qqplot(response_vals,line = 'q')\n",
    "        else:\n",
    "            distribution = distribution\n",
    "            fig = sm.qqplot(response_vals,dist = stats.gamma,distargs = (4,),line = 'q')\n",
    "        \n",
    "        fig.suptitle(var_name)\n",
    "        fig.show()\n",
    "    \n",
    "\n",
    "    def draw_kdeplot(self,ax_name, group_name, var_name: str):\n",
    "        \"\"\"\n",
    "        Visualize univariate distributions.\n",
    "        Uses Seaborn KDE plots.\n",
    "        Hue = assessment time point.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ax_name:    int\n",
    "            axis to plot on (for subplots)\n",
    "        group_name: str\n",
    "            group to plot for\n",
    "        var_name:   str\n",
    "            feature to plot\n",
    "        \n",
    "        Draws KDE plot using seaborn.\n",
    "        \"\"\"\n",
    "        df = self.in_df.loc[\n",
    "                            (self.in_df[self.group_col] == group_name),\n",
    "                            var_name\n",
    "                            ]\n",
    "        fig = sns.kdeplot(\n",
    "                            df, fill = True,\n",
    "                            hue = self.in_df[self.time_col],\n",
    "                            palette = 'crest',\n",
    "                            ax = ax_name,\n",
    "                            legend = True,\n",
    "                            #warn_singular = False\n",
    "                            )\n",
    "\n",
    "    def draw_mean_trajectories(self,var_name: str, colors, markers):\n",
    "        \"\"\"\n",
    "        Draw mean trajectories for feature var_name.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        var_name:   str\n",
    "            feature to plot\n",
    "        colors: list[str]\n",
    "            colors to use for plot (one per group)\n",
    "        markers:    list[str]\n",
    "            markers to use for pllot (one per group)\n",
    "        \"\"\"\n",
    "        fig_mean, ax = plt.subplots(figsize=(6, 6))\n",
    "        fig_mean = interaction_plot(\n",
    "                                    x=self.in_df[self.time_col],\n",
    "                                    trace=self.in_df[self.group_col],\n",
    "                                    response=self.in_df[var_name],\n",
    "                                    colors=colors,\n",
    "                                    markers=markers,\n",
    "                                    ms=10,\n",
    "                                    ax=ax,\n",
    "                                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_normality(data_to_check):\n",
    "    \"\"\" \n",
    "    Perform Anderson-Darling test to check normality.\n",
    "    H0: The data follow a normal distribution.\n",
    "    Critical values are for the following significance levels:\n",
    "    15%, 10%, 5%, 2.5%, 1%\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_to_check:  array-like\n",
    "        data to perform normality test on.\n",
    "    \n",
    "    \"\"\"\n",
    "    res = stats.anderson(data_to_check)\n",
    "    sig, crit = res.significance_level,res.critical_values\n",
    "    for i in range(len(sig)):\n",
    "        if res.statistic<sig[i]:\n",
    "            print(f'{sig[i],crit[i]}The data follow the specified distribution: failed to reject H0.')\n",
    "        else:\n",
    "            print(f'{sig[i],crit[i]} The data do not follow the specified distribution: H0 rejected.')\n",
    "\n",
    "def model_checks(data_df, columns_to_use, y_to_use,model_res, mod_type):\n",
    "    \"\"\"\n",
    "    Model diagnostics\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df:    pandas DataFrame\n",
    "        input dataframe\n",
    "    columns_to_use: str\n",
    "        names of columns containing predictors to be used in the analysis\n",
    "    y_to_use:   str\n",
    "        names of outcome column\n",
    "    model_res:    \n",
    "        statsmodel results\n",
    "    mod_type:   str\n",
    "        modelling approach used (eg LMM, GEE)\n",
    "\n",
    "    -------\n",
    "    \"\"\"\n",
    "    mod_res_df = data_df.loc[:,columns_to_use]\n",
    "    drop_inds = mod_res_df.loc[mod_res_df[y_to_use].isna(),:].index.values\n",
    "    mod_res_df = mod_res_df.drop(labels = drop_inds, axis = 0)\n",
    "    mod_res_df['fitted'] = model_res.fittedvalues\n",
    "    if mod_type == 'GEE':\n",
    "        mod_res_df['residuals'] = model_res.resid_deviance\n",
    "        a = mod_res_df.residuals.values\n",
    "        a.sort()\n",
    "\n",
    "        # half normal plot\n",
    "        fig = plt.figure()\n",
    "        res = stats.probplot(a,dist = stats.halfnorm,sparams = (-0.18,10), plot=plt)\n",
    "        plt.show()\n",
    "        g = sns.lmplot(x = \"day\", y = \"residuals\", hue = \"Group\", data = mod_res_df)\n",
    "        g = (g.set_axis_labels(\"Day [#]\", \"Residuals\"))\n",
    "\n",
    "    else:\n",
    "        mod_res_df['residuals'] = model_res.resid\n",
    "        # qq plot:\n",
    "        fig = sm.qqplot(mod_res_df.residuals)\n",
    "\n",
    "    # plotting fitted against residuals\n",
    "    g = sns.lmplot(x = \"fitted\", y = \"residuals\", hue = \"Group\", data = mod_res_df)\n",
    "    g = (g.set_axis_labels(\"Predicted score\", \"Residuals\"))\n",
    "\n",
    "\n",
    "def scale_scores(in_df,col_names, scaler_type):\n",
    "    \"\"\" \n",
    "    Scale questionnaire scores.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df:  pandas Dataframe\n",
    "        Dataframe to operate on.\n",
    "    col_names:  list\n",
    "        list of column names to operate on.\n",
    "    scaler_type:  \n",
    "        sklearn scaler to use (e.g. StandardScaler())\n",
    "    \"\"\"\n",
    "    scaler = scaler_type\n",
    "    for col in col_names:\n",
    "        in_df[col+'_scaled'] = scaler.fit_transform(in_df[col].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataframe for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_df = pd.read_csv(os.path.join(writing_dir, 'writing_df.csv'))\n",
    "mcq_df = writing_df.iloc[:,30:-5]\n",
    "#mcq_df = writing_df.filter(regex = re.compile('my memory',re.IGNORECASE),axis = 1)\n",
    "\n",
    "# reset column names so they are easier to work with.\n",
    "mcq_key = [\n",
    "            'clarity_dim_clear', 'clarity_visual_details','clarity_vivid',\n",
    "            'clarity_sketchy_clear','clarity_order_events_confusing_clear',\n",
    "            'clarity_remember_hardly_well',\n",
    "            'sensory_involves_sounds','sensory_involves_touch','sensory_involves_taste',\n",
    "            'sensory_involves_smell',\n",
    "            'context_location_clear_vague',\n",
    "            'context_spatial_arrangement_objects','context_spatial_arrangement_people',\n",
    "            'memory_time_vague_clear'\n",
    "            ]\n",
    "\n",
    "mcq_df.columns = mcq_key\n",
    "\n",
    "for col in ['id','day','Group']:\n",
    "    mcq_df[col] = writing_df[col]\n",
    "\n",
    "# set multi index for easier scoring\n",
    "tot_score_df = mcq_df.set_index(['id','day'])\n",
    "\n",
    "# select categories for MCQ scoring\n",
    "cats = ['clarity','sensory','context']\n",
    "for cat in cats:\n",
    "    cat_vars = mcq_df.filter(like = cat, axis = 1).columns\n",
    "    score_quest(tot_score_df,'_'.join(['score', cat]), list(cat_vars.values))\n",
    "\n",
    "# add a column for total score\n",
    "tot_score_items = mcq_df.columns[:-3]\n",
    "score_quest(tot_score_df,'score_total',list(tot_score_items.values))\n",
    "\n",
    "# revert back to 'regular' index bc it's easier to work with\n",
    "#tot_score_df = tot_score_df.reset_index()\n",
    "\n",
    "writing_df = writing_df.set_index(['id','day'])\n",
    "writing_df = pd.concat([writing_df,tot_score_df.drop(labels = 'Group',axis = 1)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = Data_Explorer(tot_score_df,'Group','day')\n",
    "groups = tot_score_df.Group.unique()\n",
    "cols_to_process = list(tot_score_df.filter(like = 'score',axis = 1).columns.values)\n",
    "cols_to_process.extend(['memory_time_vague_clear'])\n",
    "for col in cols_to_process:\n",
    "    de.nan_zero_ids(col)\n",
    "    de.visualize_dist(col)\n",
    "    de.draw_mean_trajectories(col,['r','b','g'],['x','*','+'])\n",
    "    fig,ax = plt.subplots(len(groups),1,figsize = (15,16))\n",
    "    for i,group_name in enumerate(groups):\n",
    "        de.draw_kdeplot(ax[i],group_name, col)\n",
    "        ax[i].title.set_text(group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now checking for score: score_clarity\n",
      "(15.0, 0.571)The data follow the specified distribution: failed to reject H0.\n",
      "(10.0, 0.651)The data follow the specified distribution: failed to reject H0.\n",
      "(5.0, 0.781) The data do not follow the specified distribution: H0 rejected.\n",
      "(2.5, 0.911) The data do not follow the specified distribution: H0 rejected.\n",
      "(1.0, 1.083) The data do not follow the specified distribution: H0 rejected.\n",
      "\n",
      "Now checking for score: score_sensory\n",
      "(15.0, 0.571)The data follow the specified distribution: failed to reject H0.\n",
      "(10.0, 0.651)The data follow the specified distribution: failed to reject H0.\n",
      "(5.0, 0.781)The data follow the specified distribution: failed to reject H0.\n",
      "(2.5, 0.911) The data do not follow the specified distribution: H0 rejected.\n",
      "(1.0, 1.083) The data do not follow the specified distribution: H0 rejected.\n",
      "\n",
      "Now checking for score: score_context\n",
      "(15.0, 0.571)The data follow the specified distribution: failed to reject H0.\n",
      "(10.0, 0.651)The data follow the specified distribution: failed to reject H0.\n",
      "(5.0, 0.781) The data do not follow the specified distribution: H0 rejected.\n",
      "(2.5, 0.911) The data do not follow the specified distribution: H0 rejected.\n",
      "(1.0, 1.083) The data do not follow the specified distribution: H0 rejected.\n",
      "\n",
      "Now checking for score: score_total\n",
      "(15.0, 0.571)The data follow the specified distribution: failed to reject H0.\n",
      "(10.0, 0.651)The data follow the specified distribution: failed to reject H0.\n",
      "(5.0, 0.781)The data follow the specified distribution: failed to reject H0.\n",
      "(2.5, 0.911)The data follow the specified distribution: failed to reject H0.\n",
      "(1.0, 1.083) The data do not follow the specified distribution: H0 rejected.\n",
      "\n",
      "Now checking for score: memory_time_vague_clear\n",
      "(15.0, 0.571) The data do not follow the specified distribution: H0 rejected.\n",
      "(10.0, 0.651) The data do not follow the specified distribution: H0 rejected.\n",
      "(5.0, 0.781) The data do not follow the specified distribution: H0 rejected.\n",
      "(2.5, 0.911) The data do not follow the specified distribution: H0 rejected.\n",
      "(1.0, 1.083) The data do not follow the specified distribution: H0 rejected.\n"
     ]
    }
   ],
   "source": [
    "# check normality\n",
    "for col in cols_to_process:\n",
    "    print(f\"\\nNow checking for score: {col}\")\n",
    "    check_normality(tot_score_df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_df = writing_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get polarity scores\n",
    "pols = ['compound','pos','neg']\n",
    "for pol in pols:\n",
    "    writing_df = get_polarity(writing_df,'writing',pol)\n",
    "\n",
    "sent_types = ['sentiment_pos_vader','sentiment_neg_vader', 'sentiment_compound_vader']\n",
    "scaler = RobustScaler()\n",
    "writing_df = scale_features(writing_df, sent_types, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_cats = ['_'.join([f,'scaled']) for f in sent_types]\n",
    "cols_to_process = list(tot_score_df.filter(like = 'score',axis = 1).columns.values)\n",
    "for col in cols_to_process:\n",
    "    for sent in sentiment_cats:\n",
    "        formula = ''.join([col,\"~\",\"C(day,Treatment(1)) * C(Group) *\", sent])\n",
    "        model_mcq = smf.mixedlm(\n",
    "                                formula,\n",
    "                                writing_df[writing_df.Group.isin(['EW','EWRE'])],\n",
    "                                groups='id',missing = 'drop'\n",
    "                                ).fit()\n",
    "        #print(model_mcq.summary())\n",
    "        with open(os.path.join(output_dir,''.join([\"model_results_\",col,\"_\",sent, \".txt\"])), \"w\") as f:\n",
    "                f.write(f\"{col}:\\n{model_mcq.summary()}\\n\")\n",
    "        #model_checks(tot_score_df[tot_score_df.Group.isin(['EW','EWRE'])],\n",
    "     #              ['Group','day',col],col,model_mcq,'LMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store mcq scores as csv file\n",
    "tot_score_df.to_csv(os.path.join(output_dir,'mcq_scored.csv'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48186e61764c8c514947f0ef500accf59797b98e64cdc910e21ec2975c1f1025"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
